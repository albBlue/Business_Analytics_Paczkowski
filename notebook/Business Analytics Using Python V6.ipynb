{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red>Introduction to Business Analytics:<br>Using Python for Better Business Decisions</font>\n",
    "\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* **Walter R. Paczkowski**, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    * [See My Books](https://www.amazon.com/-/e/B084KK4SF5?ref_=pe_1724030_132998070)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide Set-up\n",
    "\n",
    "This code sets up the presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Slide code\n",
    "##\n",
    "from IPython.display import Image\n",
    "def slide(what):\n",
    "    display( Image( \"../Slides/BA_Page_\" + what + \".png\", width = 50, height = 50, retina = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [**_Helpful Background_**](#Helpful-Background)\n",
    "    1. [About this Notebook](#About-this-Notebook)\n",
    "    2. [Helpful Online Tutorials](#Helpful-Online-Tutorials)\n",
    "    3. [Helpful Must-Read Book](#Helpful-Must-Read-Book)\n",
    "2. [**_Lesson I Introduction to Business Anaytics_**](#Lesson-I-Introduction-to-Business-Anaytics)\n",
    "3. [**_Lesson II Simple Analytics: Understanding and Preparing Your Data_**](#Lesson-II-Simple-Analytics:-Understanding-and-Preparing-Your-Data)\n",
    "    1. [II.1 Documenting Your Data and Workflow: Best Practices](#II.1-Documenting-Your-Data-and-Workflow:-Best-Practices)\n",
    "        1. [II.1.1 Documenting Your Data](#II.1.1-Documenting-Your-Data)\n",
    "        2. [Exercise II.1](#Exercise-II.1)\n",
    "        3. [II.1.2 Documenting Your Workflow](#II.1.2-Documenting-Your-Workflow)\n",
    "        4. [II.1.3 Documenting Your Code](#II.1.3-Documenting-Your-Code)\n",
    "    2. [II.2 Importing Python Packages](#II.2-Importing-Python-Packages)\n",
    "        1. [II.2.1 Introduction to Python Packages](#II.2.1-Introduction-to-Python-Packages)\n",
    "        2. [II.2.2 Loading Packages](#II.2.2-Loading-Packages)\n",
    "        3. [II.2.3 Accessing a Function in a Package](#II.2.3-Accessing-a-Function-in-a-Package)\n",
    "    3. [II.3 Importing Your Data Into Pandas](#II.3-Importing-Your-Data-Into-Pandas)\n",
    "        1. [II.3.1 Set Data Path](#II.3.1-Set-Data-Path)\n",
    "        2. [II.3.2 Importing Data](#II.3.2-Importing-Data)\n",
    "    4. [II.4 Checking Your Data](#II.4-Checking-Your-Data)\n",
    "    5. [II.5 Manipulating Your Data](#II.5-Manipulating-Your-Data)\n",
    "        1. [II.5.1 Creating Variables](#II.5.1-Creating-Variables) \n",
    "        2. [Exercise II.2](#Exercise-II.2)\n",
    "        3. [Exercise II.3](#Exercise-II.3)\n",
    "        4. [Exercise II.4](#Exercise-II.4)\n",
    "        5. [Exercise II.5](#Exercise-II.5)\n",
    "        6. [II.5.2 Merge or Join DataFrames](#II.5.2-Merge-or-Join-DataFrames)\n",
    "        7. [Exercise II.6](#Exercise-II.6)\n",
    "    6. [II.6 Summary Statistics for Your Data](#II.6-Summary-Statistics-for-Your-Data)            \n",
    "        1. [Exercise II.7](#Exercise-II.7)\n",
    "    7. [II.7 What is Next?](#II.7-What-is-Next?)\n",
    "4. [**_Lesson III Data Visualization for Insight_**](#Lesson-III-Data-Visualization-for-Insight)\n",
    "    1. [III.1 Look at the Distribution of Your Data](#III.1-Look-at-the-Distribution-of-Your-Data)\n",
    "        1. [III.1.1 Histograms](#III.1.1-Histograms)\n",
    "            1. [Exercise III.1](#Exercise-III.1)\n",
    "        2. [III.1.2 Boxplots](#III.1.2-Boxplots)\n",
    "            1. [Exercise III.2](#Exercise-III.2) \n",
    "    2. [III.2 Look for Relationships in Your Data](#III.2-Look-for-Relationships-in-Your-Data)\n",
    "        1. [III.2.1 Transformation for Better Interpretation](#III.2.1-Transformation-for-Better-Interpretation)\n",
    "        2. [III.2.2 Enhancing the Scatter Plot](#III.2.2-Enhancing-the-Scatter-Plot)\n",
    "        3. [III.2.3 Working with *Large-N* Data](#III.2.3-Working-with-Large-N-Data)    \n",
    "            1. [III.2.3.1 Random Sampling](#III.2.3.1-Random-Sampling)\n",
    "            2. [Exercises III.3](#Exercise-III.3)\n",
    "            3. [III.2.3.2 Contour Plot](#III.2.3.2-Contour-Plot)\n",
    "            4. [III.2.3.3 Hex Bin Plot](#III.2.3.3-Hex-Bin-Plot)\n",
    "            5. [III.2.3.4 Lowess Curve](#III.2.3.4-Lowess-Curve)\n",
    "            6. [Exercises III.4](#Exercise-III.4)\n",
    "    3. [III.3 Look for Trends in Your Data](#III.3-Look-for-Trends-in-Your-Data)         \n",
    "    4. [III.4 Look for Patterns in Your Data](#III.4-Look-for-Patterns-in-Your-Data)\n",
    "    5. [III.5 Look for Anomalies in Your Data](#III.5-Look-for-Anomalies-in-Your-Data) \n",
    "    6. [III.6 What is Next?](#III.6-What-is-Next?)        \n",
    "5. [**_Lesson IV Predictive Modeling: Introduction to Machine Learning_**](#Lesson-IV-Predictive-Modeling:-Introduction-to-Machine-Learning)\n",
    "    1. [IV.1 Comparing and Contrasting Prediction and Forecasting](#IV.1-Comparing-and-Contrasting-Prediction-and-Forecasting)\n",
    "    2. [IV.2 Steps for Predictive Modeling](#IV.2-Steps-for-Predictive-Modeling)\n",
    "        1. [IV.2.1 Steps for Predictive Modeling: Train/Test Split Data](#IV.2.1-Steps-for-Predictive-Modeling:-Train/Test-Split-Data)\n",
    "            1. [Exercise IV.1](#Exercise-IV.1)\n",
    "            2. [Exercise IV.2](#Exercise-IV.2)\n",
    "        2. [IV.2.2 Steps for Predictive Modeling: Train a Model](#IV.2.2-Steps-for-Predictive-Modeling:-Train-a-Model)\n",
    "           1. [Case I Continuous Dependent Variable: OLS Regression](#Case-I-Continuous-Dependent-Variable:-OLS-Regression)\n",
    "              1. [Exercise IV.3](#Exercise-IV.3)           \n",
    "              2. [Case I Analyze the Results](#Case-I-Analyze-the-Results)\n",
    "              3. [Exercise IV.4](#Exercise-IV.4)\n",
    "              4. [Case I Predict with the Model](#Case-I-Predict-with-the-Model)\n",
    "              5. [Exercise IV.5](#Exercise-IV.5)\n",
    "           2. [Case II Binary Dependent Variable: Logistic Regression](#Case-II-Binary-Dependent-Variable:-Logistic-Regression)\n",
    "              1. [Case II Create Your Data](#Case-II-Create-Your-Data)\n",
    "              2. [Case II Train a Model](#Case-II-Train-a-Model)\n",
    "              3. [Case II Predict with the Model](#Case-II-Predict-with-the-Model)\n",
    "           2. [Case III Constants: Decision Trees](#Case-III-Constants:-Decision-Trees)\n",
    "              1. [Case III Check Model Accuracy](#Case-III-Check-Model-Accuracy)\n",
    "              2. [Case III Display the Tree](#Case-III-Display-the-Tree)\n",
    "              3. [Exercise IV.6](#Exercise-IV.6)\n",
    "6. [**_Lesson V Summary and Wrap-up_**](#Lesson-V-Summary-and-Wrap\\-up)\n",
    "7. [**_Contact Information_**](#Contact-Information)\n",
    "8. [**_Appendix_**](#Appendix)\n",
    "    1. [Appendix I.1 Jupyter Notebooks: Overview](#Appendix-I.1-Jupyter-Notebooks:-Overview)\n",
    "    2. [Appendix I.2 Different Ways to Import Data Into Pandas](#Appendix-I.2-Different-Ways-to-Import-Data-Into-Pandas)\n",
    "    3. [Appendix I.3 Some Additional Information on Checking Your Data](#Appendix-I.3-Some-Additional-Information-on-Checking-Your-Data)\n",
    "        1. [Task \\#1 Display the First Few Records of Your DataFrame](#Task-\\#1-Display-the-First-Few-Records-of-Your-DataFrame)\n",
    "        2. [Task \\#2 Check the Shape of Your DataFrame](#Task-\\#2-Check-the-Shape-of-Your-DataFrame)\n",
    "        3. [Task \\#3 Check the Column Names in Your DataFrame](#Task-\\#3-Check-the-Column-Names-in-Your-DataFrame)\n",
    "        4. [Task \\#4 Check for Missing Data in Your DataFrame](#Task-\\#4-Check-for-Missing-Data-in-Your-DataFrame)\n",
    "    4. [Appendix I.4 Miscellaneous Pandas DataFrame Column Manipulations](#Appendix-I.4-Miscellaneous-Pandas-DataFrame-Column-Manipulations)\n",
    "        1. [Deleting Columns](#Deleting-Columns)\n",
    "    5. [Appendix I.5 Correlation Analysis](#Appendix-I.5-Correlation-Analysis)\n",
    "    6. [Appendix II.1 Data Visualization](#Appendix-II.1-Data-Visualization)\n",
    "        1. [Additional Histogram Methods](#Additional-Histogram-Methods)\n",
    "        2. [Additional Boxplot Methods](#Additional-Boxplot-Methods)\n",
    "        3. [Additional Scatter Plot Methods](#Additional-Scatter-Plot-Methods)\n",
    "           1. [Categorical Variable](#Categorical-Variable)\n",
    "           2. [Panel Plots](#Panel-Plot)\n",
    "           3. [Combining Scatter Plots and Histograms](#Combining-Scatter-Plots-and-Histograms)\n",
    "           4. [Pairwise Scatter Plots](#Pairwise-Scatter-Plots)\n",
    "           5. [Contour Plots with Density Functions](#Contour-Plots-with-Density-Functions)\n",
    "        4. [Additional Time Series Plot Methods](#Additional-Time-Series-Plot-Methods)\n",
    "    7. [Appendix III.1 Extra Material for Predictive Modeling](#Appendix-III.1-Extra-Material-for-Predictive-Modeling)\n",
    "        1. [Check OLS Model for Multicollinearity](#Check-OLS-Model-for-Multicollinearity)\n",
    "        2. [Case I Model Portfolio](#Case-I-Model-Portfolio)\n",
    "        3. [Case II Model Portfolio](#Case-II-Model-Portfolio)\n",
    "    8. [Appendix Complete Data Dictionary](#Appendix-Complete-Data-Dictionary)\n",
    "9. [**_Exercise Solutions_**](#Exercise-Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Background\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Notebook\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This notebook accompanies the PDF presentation\n",
    "\n",
    "> ***Business Analytics: Using Python for Better Business Decisions***\n",
    "\n",
    "by Walter R. Paczkowski, Ph.D. (2020).  There is more content and commentary in this notebook than in the presentation deck.  Nonetheless, the two complement each other and so should be studied together.  Every effort has been made to use the same key slide titles in the presentation deck and this notebook which will help your studying.  For your convenience, most of the presentation deck slides have been incorporated into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Online Tutorials\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"http://docs.python.org/2/tutorial/\" target=\"_parent\">Python Tutorial</a>\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\" target=\"_parent\">Pandas Tutorial</a>\n",
    "\n",
    "* <a href=\"https://seaborn.pydata.org/tutorial.html\" target=\"_parent\">Seaborn Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.statsmodels.org/stable/index.html\" target=\"_parent\">Statsmodels Tutorial</a>\n",
    "\n",
    "\n",
    "### Helpful Must-Read Book\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&tag=quantpytho-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491957662&linkId=8c3bf87b221dbcd8f541f0db20d4da83\" target=\"_parent\">Main Pandas go-to book: </a> *Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython* (2nd Edition) by Wes McKinney.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson I Introduction to Business Anaytics\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '003' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '005' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '007' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '009' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '010' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '011' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '012' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson II Simple Analytics: Understanding and Preparing Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '014' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '016' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '017' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Documenting Your Data and Workflow: Best Practices\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '019' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.1 Documenting Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The first task in any data analysis is data documentation in a *Data Dictionary*.\n",
    "\n",
    "A data dictionary contains *metadata* which are data about the data.  Metadata can be anything that helps you understand the data you're using.  Based on [Wikipedia](http://en.wikipedia.org/wiki/Metadata), metadata are information about the distinct data items, such as:\n",
    "\n",
    "> 1. means of creation;\n",
    "> 2. purpose of the data;\n",
    "> 3. time and date of creation;\n",
    "> 4. creator/author/keeper of the data;\n",
    "> 5. placement on a network (electronic form);\n",
    "> 6. where the data were created;\n",
    "> 7. what standards were used to create the data; and \n",
    "> 8. etc.\n",
    "\n",
    "I'll restrict the metadata to:\n",
    "\n",
    "> 1. Variable name;\n",
    "> 2. Possible values or value ranges;\n",
    "> 3. Source; and \n",
    "> 4. Mnemonic.\n",
    "\n",
    "The mnemonic is the label used in data files and statistical and modeling output.  \n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "\n",
    "A complete data dictionary is [here](#Appendix-Complete-Data-Dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "We will soon import customer specific data that has four columns: \n",
    "\n",
    "> 1. CID: the customer ID;\n",
    "> 2. State: the 50 US states plus Washington, DC;\n",
    "> 3. ZIP: the 5-digit US ZIP (postal) code; and\n",
    "> 4. Region: the marketing region which corresponds to the four US Census Regions (Midwest, Northeast, South, and West).\n",
    "\n",
    "Create a Data Dictionary assuming this data come from the marketing department.  Use the labels *CID, State, ZIP*, and *Region* as the mnemonics.  Enter the Data Dictionary in a Markdown cell.\n",
    "\n",
    "[See Solution](#Solution-II.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the Data Dictionary here:\n",
    "\n",
    "| Variable                     | Values                              | Source       | Mnemonic          |\n",
    "|------------------------------|-------------------------------------|--------------|-------------------|\n",
    "|                              |                                     |              |                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.2 Documenting Your Workflow\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Documenting your workflow is as important as documenting your data.  This documentation will enable you to reproduce your work and make it easier for a colleague to follow what you did.  The *Jupyter notebook* paradigm is the best platform for this documentation.  You will see many examples of this throughout this course. See [here](#Appendix-I.1-Jupyter-Notebooks:-Overview) for an overview of Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.3 Documenting Your Code\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Always add comments to your code so that you can later recall why you did something.  Comments are added using a hash or pound sign (*#*).  I usually use two pound signs.  Anything following a pound sign is treated as a comment and is thus ignored.  You can add a comment anywhere on a line.  \n",
    "\n",
    "Adding comments can be viewed as writing a piece of prose.  This is referred to as *literate programming.*  See [here](https://en.wikipedia.org/wiki/Literate_programming) for a discussion on literate programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Importing Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Python is a powerful programming language that allows you to perform all standard programming operations in a clear and consistent manner.  Its strength, adhered to by Python programmers, is a coding format that emphasizes readable code.  Indentation is the primary way to accomplish this.  Also, its strength is based on a very wide array of *packages* or *modules* or *libraries*.  Packages perform analysis or data manipulation operations.  There are many packages, each one providing a special set of analysis tools so a package can be viewed as a container of functions.  Sometimes a package contains smaller, more specialized packages so a grand package could be a container for smaller ones.  You will see how to access and use packages and subset packages in this and other lessons.\n",
    "\n",
    "Pandas is a data manipulation and graphing package with a lot of capabilities.  It will be used extensively in these lessons.  Seaborn is a scientific graphing package that is intuitive to use.  Although Pandas has visualization methods, Seaborn is preferred because of its quality, extent, and easier syntax.  Both packages use Matplotlib for base graphing functions.  Statsmodels has an array of statistical modeling functions, only a few of which will be used in these lessons.  Numpy and Matplotlib are base packages for Pandas, Seaborn, and Statsmodels.  Except for a few functions, Numpy and Matplotlib will not be used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1 Introduction to Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '022' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2 Loading Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You have to load a package before you can use it.  Loading is done using an *import* command.  The alias is assigned when you import the package.  I recommend loading all the basic packages at once at the beginning of your notebook so you do not have to search for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Data Management <===\n",
    "##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "##\n",
    "## ===> Visualization <===\n",
    "##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "## Set the seaborn grid style.  The dot between the seaborn alias,\n",
    "## \"sns\", and the set() function connects or \"chains\" the alias and the method.\n",
    "##\n",
    "sns.set()\n",
    "##\n",
    "## Set an option for the number of Pandas columns to display.  Eight in this case.\n",
    "## \n",
    "pd.set_option( 'display.max_columns', 8 )\n",
    "##\n",
    "## ===> Modeling <===\n",
    "##\n",
    "## Import train_test_split package from sklearn\n",
    "##\n",
    "from sklearn.model_selection import train_test_split\n",
    "##\n",
    "## For modeling, notice the new import command for\n",
    "## the formula API and the summary option\n",
    "##\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "##\n",
    "## Import the r2_score function from the sklearn metrics package\n",
    "##\n",
    "from sklearn.metrics import r2_score\n",
    "##\n",
    "## Import confusion functions for classification\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "##\n",
    "## Import decision tree classifier functions\n",
    "##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "##\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "##\n",
    "## Some packages are needed for decision trees:\n",
    "## Some additional packages are needed to plot a decision tree:\n",
    "## - graphviz\n",
    "## - pydotplus\n",
    "## Both packages may have to be installed before they can be used.  \n",
    "## Use the operating system to do this.\n",
    "##\n",
    "import os\n",
    "##!{sys.executable} -m pip install graphviz\n",
    "##!{sys.executable} -m pip install pydotplus\n",
    "##\n",
    "## Tell Python where the graphviz package is load; then load it.\n",
    "##\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "##\n",
    "## Load the following packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import graphviz\n",
    "import pydotplus\n",
    "##\n",
    "## Import needed tree packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "This code block loads the necessary Python packages for this course.  I recommend setting options, such as those for graphs and print, as was done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3 Accessing a Function in a Package\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "A function in a package can be accessed by telling Python the package where the function is located and, of course, the name of the function.  These two operations are done with one statement by *chaining* the package name and the function name.  The chain is formed by connecting the package name and the function name by inserting a dot between the two.  Usually, the package alias is used for improved readability.  An example of a chained command is:\n",
    "\n",
    "> *pd.read_csv( 'lesson1.csv' )*\n",
    "\n",
    "where \"*pd*\" is the alias for Pandas and \"*read_csv*\" is a Pandas function that reads a *CSV* file (\"lesson1.csv\" in this example). Notice the dot(\".\") between the alias and the function name.  The dot is the chaining operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Importing Your Data Into Pandas\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Before you can begin any work, you must first import and examine the structure of your data.  This structure is a rectangular array or matrix or, in Pandas terminology, a *DataFrame*.  When you import your data using Pandas, the imported data immediately goes into a DataFrame.  This is very convenient because Seaborn and StatsModels functions recognize these DataFrames.\n",
    "\n",
    "Pandas provides a set of very flexible import functions.  Which one you should use depends on your data format.  Some typical formats and relevant functions are:\n",
    "\n",
    "| Data Format | Pandas Import Function |\n",
    "|-------------|------------------------|\n",
    "| CSV         | read_csv               |\n",
    "| Excel       | read_excel             |\n",
    "| Clipboard   | read_clipboard         |\n",
    "| JSON        | read_json              |\n",
    "| SAS         | read_sas               |\n",
    "| HDF5        | read_hdf               |\n",
    "\n",
    "The *HDF5* format is especially important for many Business Analytics applications where the datasets are large and complex.  *HDF* stands for *Hierarchical Data Format*.  \"*HDF5 is a unique technology suite that makes possible the management of extremely large and complex data collections.*\"  I will not use this format in this course.  See [here](https://portal.hdfgroup.org/display/support) for information on *HDF5* and [here](https://www.kaggle.com/diegovicente/a-short-introduction-to-hdf5-files) for a short introduction to Pandas and *HDF5*.\n",
    "\n",
    "I will first import a *CSV* formatted file.  The package alias must be \"chained\" with the *read_csv* import function, otherwise Python will not know where to find the read function. \n",
    "\n",
    "When you import data, you must always specify the file path so Pandas can find the file.  If the data file is in the same directory as the notebook, then a path is unnecessary since Pandas always begins a search in the same directory as the notebook.  Otherwise, you have to specify the path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.1 Set Data Path\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is best practice to define paths in one location.  This makes error finding and changes easier.  Define the data path as shown here; keep the format *r'path'* if necessary.  Remember, if your data are in the same directory as your notebook, then a path is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set data path\n",
    "##\n",
    "path = r'../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The path is specified using the string literal: *r'../Data/'*.  The *r* at the beginning of the string tells the Python interpreter to treat this string as raw or literal text that is not to be changed.  Without the *r*, the interpreter could treat any backslashes as escape characters which would change the meaning of the string.  Even though forward slashes are used in this code block, it is good practice to avoid issues and use the *r*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.2 Importing Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "An example of data import is shown below.  Several more are shown in the Appendix [here](#Appendix-I.2-Different-Ways-to-Import-Data-Into-Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and import or read the data.\n",
    "##\n",
    "file = 'orders.csv'\n",
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "df_orders = pd.read_csv( path + file, parse_dates = [ 'Tdate' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The *CSV* file is specified as a string and then the path and file strings are concatenated using the \"+\" symbol.  The *parse_dates* argument says to treat the variable *Tdate* as a date variable.  This variable is the date of a transaction.  Date variables are stored and handled differently in Pandas and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, count the number of unique *CID*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## How many unique CIDs are available?\n",
    "##\n",
    "data = len( df_orders.CID.unique() )\n",
    "print( 'Number of unique CIDs: {}'.format( data ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *unique()* method extracts the unique *CID*s from the orders DataFrame.  The *len* function then counts the number of unique *CID*s.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There are 779 unique *CID*s in this data set.  You will see this number quite often in succeeding lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4 Checking Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '029' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 1: Display first few records.\n",
    "##\n",
    "df_orders.head().style.set_caption( 'Orders Data' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The \"head\" method is attached to a DataFrame when you create it.  The default is to display the first five records.  You could use *n = 10* as an argument to display the first 10 records: *df.head( n = 10 )*.  You could display the last five records using the *tail* method.  The default is also five which could be changed as for the *head* method.  For both *head* and *tail*, the number of columns displayed is set using *pd.set_option( 'display.max_columns', 8 )* as was done in the package loading section above.  The *head* and *tail* methods are chained to the DataFrame name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 2: Check the shape of the data\n",
    "##\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_orders.shape[ 0 ], cols = df_orders.shape[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*shape* is an attribute of the DataFrame so it does not require parentheses; it does not have any arguments.  Functions and methods have arguments (which may be defaults) so parentheses are required.  The *shape* attribute is chained to the DataFrame name.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The *shape* attribute returns the number of rows and the number of columns in that order.  The *orders* DataFrame has 70,270 *rows* or *records* or *observations* and 14 *columns* or *variables* or *features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 3: Check the column names\n",
    "##\n",
    "print( 'The column labels in the DataFrame:\\n{}'.format( df_orders.columns ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*columns* is an attribute of the DataFrame.  As an attribute, it does not require parentheses since an attribute is not callable so it has no arguments.\n",
    "\n",
    "**_Interpretaion_**\n",
    "\n",
    "When checking the column names, be sure there are no white spaces before and after the name.  White spaces can (and will) cause problems because your tendency will be to write a column name without the leading and trailing white spaces; Python will then not recognize the name.  If you see leading and trailing white spaces, you can remove them using the following:\n",
    "\n",
    "> *df_orders.columns = df_orders.columns.str.strip()*\n",
    "\n",
    "where *str* is the string package which is part of the base Python kernel and automatically loaded with Python.  You may also want to convert the column names to all lower case:\n",
    "\n",
    "> *df_orders.columns = df_orders.columns.str.lower()*\n",
    "\n",
    "You could do both at once using:\n",
    "\n",
    "> *df_orders.columns = df_orders.columns.str.strip().str.lower()*\n",
    "\n",
    "Notice the use of *str* twice in this last expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 4: Check for missing data\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*info* is a method chained to the DataFrame.  It returns the number of non-missing records for each column plus data types: object (i.e., text string), floating point numbers, integers, and datetime.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "There are 14 columns with 10 having 70,270 nonmissing values while the last four have less than 70,270 so they have mising values.  For example, *Ddisc* (the dealer discount based on the Data Dictionary) has 70,262 records so 8 are missing. \n",
    "\n",
    "See the Appendix [here](#Appendix-I.3-Some-Additional-Information-on-Checking-Your-Data) for more information about checking your data.\n",
    "\n",
    "This dataset is moderately large.  If memory becomes an issue with very large datasets, then an argument for *read_csv* can be used to read in chunks of data.  The argument is *chunksize = XXX* where *XXX* is the number of records to read in each chunk.  This is an advanced topic.  See [here](https://stackoverflow.com/questions/33642951/python-using-pandas-structures-with-large-csviterate-and-chunksize) for some dicussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5 Manipulating Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '032' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.5.1 Creating Variables\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '034' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate total discount.\n",
    "##\n",
    "## Discounts are sometimes called \"leakages\" so the total is \n",
    "## the total leakage.\n",
    "##\n",
    "## Note: use \"axis = 1\" in the sum() function to sum across columns.\n",
    "## This allows you to do the summation even with missing values.\n",
    "##\n",
    "lst = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ lst ].sum( axis = 1 )\n",
    "##\n",
    "## Display only the discounts\n",
    "##    Create a list of what to print. \n",
    "##\n",
    "lst.append( 'Tdisc' )\n",
    "df_orders[ lst ].head().style.set_caption( 'Orders Data for Discounts' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The *sum* method has an axis argument that specifies the axis the function is to be applied on.  *axis = 0* specifies summing along the rows for each column (i.e., sum down a column) and *axis = 1* specifies summing along the columns in each row.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Notice the *NaN* values.  *NaN* stands for *Not a Number*.  These are missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The *Pocket Price* is the list price less total discounts or total leakages.  It is the amount the business \"pockets\" and is the amount the customer actually pays.  The pocket price formula is\n",
    "\n",
    "> $Pprice = Lprice \\times (1  - Tdisc)$.\n",
    "\n",
    "The *Tdisc* variable was created above.  Calculate the pocket price for the *df_orders* DataFrame and display the first five records for the list price and pocket price.\n",
    "\n",
    "[See Solution](#Solution-II.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Calculate total revenue as \n",
    "\n",
    "> $Rev = Usales \\times Pprice$ \n",
    "\n",
    "using the *df_orders* DataFrame.\n",
    "\n",
    "[See Solution](#Solution-II.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "*Contribution* and *contribution margin* are two values financial analysts often examine.  Contribution is comparable to what economists call *profit* but is more restricted in that it just refers to a product without considering any fixed or overhead costs.  Contribution is\n",
    "\n",
    "> $Con = Revenue - Material~Cost$ \n",
    "\n",
    "and contribution margin is \n",
    "\n",
    "> $CM = \\dfrac{Con}{Revenue}$.\n",
    "\n",
    "Calculate both quantities using the *df_orders* DataFrame.\n",
    "\n",
    "[See Solution](#Solution-II.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.5\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Some products are returned so another revenue number, *revenue net of returns*, is more meaningful and revealing for business decisions.  Net revenue is\n",
    "\n",
    "> $Net Revenue = (Unit Sales - Returns) \\times Pocket Price$.\n",
    "\n",
    "Calculate net revenue and call it *netRev*.  Also calculate the loss in revenue due to the returns.  The calculation is\n",
    "\n",
    "> $lostRev = Rev - netRev$. \n",
    "\n",
    "Use the *df_orders* DataFrame.  \n",
    "\n",
    "[See Solution](#Solution-II.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.5.2 Merge or Join DataFrames \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is not unusual to have data in two (or more) tables so you will need to *merge* or *join* them to get all the data you need for an analysis.  For our problem, a second data table has information on each customer and this second table must be merged with the orders table.  The merge is done on the customer *ID* (*CID*).  There are many types of joins but we will only use an *inner join* in the examples.  *Inner join* is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '037' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = 'customers.csv'\n",
    "df_cust = pd.read_csv( path + file )\n",
    "df_cust.head().style.set_caption( 'Customer DataFrame' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df_orders_cust = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "##\n",
    "df_orders_cust.head().style.set_caption( 'Orders-Customers DataFrame' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The merge function takes two arguments: the *left* table and the *right* table to merge or join.  The tables are in that order.  A third argument specifies what to merge on.  There are several options for the *on* variable.  In this example, the *on* variable is just the common key in each table: *CID*.\n",
    "\n",
    "An alternative form for the merge statement is:\n",
    "\n",
    "> *df_orders = df_orders.merge( df_cust, on = 'CID' )*\n",
    "\n",
    "An *inner join* is the default.  \n",
    "\n",
    "See the Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" target=\"_parent\">here</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\" target=\"_parent\">here</a> for extensive discussion with examples about this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Check the shape of the new DataFrame against \n",
    "## that of the orders and customers DataFrames\n",
    "##\n",
    "print( 'Shape of the orders DataFrame: {}\\n'.format( df_orders.shape ) )\n",
    "print( 'Shape of the customers DataFrame: {}\\n'.format( df_cust.shape ) )\n",
    "print( 'Shape of the merged DataFrame: {}\\n'.format( df_orders_cust.shape ) )\n",
    "##\n",
    "## Find the number of unique CIDs\n",
    "##\n",
    "data = len( df_orders_cust.CID.unique() )\n",
    "print( 'Number of unique CIDs in merged DataFrame: {}'.format( data ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "*df_orders* has 21 columns, *df_cust* has 4 columns, while the merged *df_orders_cust* has 24.  The difference of one column is the *CID* which is in both and is the linking variable; it's only included once.  Notice that the number of unique *CID*s is 779 as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise II.6\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "There is a third data set: a marketing data set that contains information for each customer about their loyalty program membership, a buyer rating provided by the sales force, and their customer satisfaction rating based on an annual customer satisfaction survey.  The marketing data are in a *csv* file named *marketing.csv*.  You have to:\n",
    "\n",
    "> 1. import the marketing data into a DataFrame (name it *df_marketing*) and\n",
    "> 2. merge the *df_orders_cust* DataFrame and this marketing DataFrame.\n",
    "\n",
    "The *CID* is the same in both data sets so it is the linking variable.  Name the final merged DataFrame *df*.\n",
    "\n",
    "[See Solution](#Solution-II.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to import the marketing data\n",
    "## Name this imported data df_marketing\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to merge the df_orders_cust and df_marketing DataFrames\n",
    "## Name the new merged DataFrame df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check the shape of df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check number of unique CIDs in df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.6 Summary Statistics for Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Summary statistics are a mainstay for starting any analysis.  Pandas has all the usual descriptve statistics.  One function, *describe()* will display the essential ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: \"describe\" is a method attached to the DataFrame so it requires ().\n",
    "## Round to 1 decimal place for readability (more decimal places are\n",
    "## unnecessay, anyway).\n",
    "##\n",
    "## Display the descriptive statistics for the discounts.\n",
    "##    First create a list of variables to display.\n",
    "##\n",
    "lst = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "df[ lst ].describe().round( 1 ).style.set_caption( 'Descriptive Statistics' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *round* function is chained to the *describe* method.  An alternative way to round is shown next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above report for the descriptive statistics is a challenge to read.  I prefer to have the statistics as the columns.  This can easily be done once you recognize that the report is just a matrix.  Matrices can be transposed which could help you read the report more easily.  Use the *T* attribute to transpose a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example of transposed matrix and alternative\n",
    "## round function use.\n",
    "##\n",
    "lst = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "round( df[ lst ].describe().T, 1 ).style.set_caption( 'Descriptive Statistics' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "From the *Five Number Summary* (min/25%/50%/75%/max), you can determine the skewness of your data.\n",
    "\n",
    "> 1. Symmetric: $(75\\% - 50\\%) = (50\\% - 25\\%)$\n",
    "> 2. Right Skewed: $(75\\% - 50\\%) > (50\\% - 25\\%)$\n",
    "> 3. Left Skewed: $(75\\% - 50\\%) < (50\\% - 25\\%)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** What is the skewness for the Dealer Discount (*Ddisc*)?\n",
    "\n",
    "**ANSWER**: right skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.7\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Using your merged orders/customers/marketing DataFrame, *df*, create a summary statistics display.  What is the skewness of the Total Discount (*Tdisc*)?\n",
    "\n",
    "[See Solution](#Solution-II.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.7 What is Next?\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "In Lesson III, I will show you how to do some basic graphing or *visualization* of your data.  This may seem more like scientific visualization than business visualization.  The latter is usually *infographics* which is not useful for gaining insight and, hence, useful Rich Information.  The former, scientific visualization, is the tool for extracting Rich Information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( \"042\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson III Data Visualization for Insight\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '044' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Specifically, you will learn to use:\n",
    "\n",
    "> 1. histograms;\n",
    "> 2. boxplots;\n",
    "> 3. scatter plots;\n",
    "> 4. contour plots; and\n",
    "> 5. hex bin plots\n",
    "\n",
    "to visualize your data.  The focus is on *scientific visualization* rather than *infographics visualization*.      \n",
    "\n",
    "**Case Study Problem**:\n",
    "<br><br>\n",
    "The product manager wanted to know about unit sales and discounts by:\n",
    "\n",
    "> 1. overall market;\n",
    "> 2. marketing region;\n",
    "> 3. customer loyalty; and\n",
    "> 4. buyer rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '046' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '047' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '049' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '051' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1 Look at the Distribution of Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '053' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.1 Histograms\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '054' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a histogram to examine the distribution of unit sales and the total discount.  Notice in the following display that a smooth line is overlayed.  This is a *kernel density estimate* (*KDE*).  You will see this again shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '055' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Histogram of unit sales\n",
    "##\n",
    "ax = sns.distplot( df.Usales )\n",
    "ax.set( title = 'Unit Sales Distribution', xlabel = 'Unit Sales', \n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Plotting a histogram is very easy.  The Seaborn *distplot* command is used with the argument set to the variable of interest.  The plot is saved in a variable called \"ax\".  Parameters such as title and labels can be passed to this variable.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The distribution is highly skewed to the right which distorts the impression of the data.  The natural log will normalize the display.  This is helpful so when you model unit sales you should use a log transformation.  This next graph shows that the distribution (on a log scale) is fairly normal.\n",
    "\n",
    "**_Recommendation_**\n",
    "    \n",
    "Use the Numpy *log1p* function.  This returns the natural log of one plus the argument: $np.log1p( x ) = log_e(1 + x)$.  The reason for using this function is to avoid cases where $x = 0$: $log(0)$ is undefined, which is meaningless, but $log( 1 ) = 0$ so you would have a meaningful number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the natural log of unit sales\n",
    "## A KDE curve is included by default\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales ) )\n",
    "ax.set( title = 'Unit Sales Distribution: Log Scale', \n",
    "       xlabel = 'Unit Sales (Natural Log)',\n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The natural log transformation changed the distribution to a more normal looking distribution.  Normality is preferred for statistical analysis for a host of reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Examine the distribution of pocket price using a histogram.  What can you conclude?  Redo using a log transformation.  Now what do you conclude?\n",
    "\n",
    "[See Solution](#Solution-III.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for pocket price.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for transformed pocket price.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.2 Boxplots\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Boxplots are the most useful visualization tool for examining distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '057' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '058' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the boxplot for total discounts\n",
    "##\n",
    "ax = sns.boxplot( y = 'Tdisc', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount', ylabel = 'Total Discount' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that the Seaborn boxplot function only has an argument for the y-axis.  In this case, the x-axis is understood.  This gives a vertical chart as shown.  However, if you change the \"y\" to \"x\", the boxplot will be horizontal: \n",
    "\n",
    "> *sns.boxplot( x = 'Tdisc', data = df )*\n",
    "\n",
    "produces a horizontal chart.  You can also use the argument *orient = \"h\"* or *orient = \"v\"* if the entire DataFrame is used.  See [here](https://seaborn.pydata.org/generated/seaborn.boxplot.html) for examples.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The Total Discount is symmetrically distributed.  This is evident by an almost mirror image above and below the center line inside the box.  The center line is the median.  This boxplot is for the entire market.  But what about regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region', ylabel = 'Total Discount', \n",
    "       xlabel = 'Marketing Regions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "In this drill-down of the total discounts by marketing regions, the Seaborn boxplot function now has two axis arguments: \n",
    "\n",
    "> 1. y-axis; and\n",
    "> 2. x-axis (*Region* in this case).\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Notice that discounts are the lowest in the Southern Region while the Midwest has a large number of very low discounts.  Also, the dispersion of the discounts in the Southern Region is small relative to that in the other three regions.  Let us drill down on the discounts to verify the differences for the Southern Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drill down on the discounts in the Southern Region\n",
    "##\n",
    "## Select the discounts for the Southern Region\n",
    "##\n",
    "lst = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "df_south = df.loc[ df.Region == 'South', lst ]\n",
    "##\n",
    "## Melt the data from wide- to long-form.\n",
    "##\n",
    "df_melt = pd.melt( df_south )\n",
    "##\n",
    "## Get summary statistics\n",
    "##\n",
    "grp = df_melt.groupby( 'variable' ).describe()\n",
    "print( 'Summary statistics for the discounts:\\n{}'.format( grp ) )\n",
    "##\n",
    "## Use a boxplot to examine the distributions.\n",
    "##\n",
    "ax = sns.boxplot( x = 'variable', y = 'value', data = df_melt )\n",
    "ax.set( title = 'Discount Distribution\\nSouthern Marketing Region', \n",
    "        xlabel = 'Type of Discount',\n",
    "        ylabel = 'Discount Amount')\n",
    "##\n",
    "## Reset the tick labels to more meaningful labels\n",
    "##\n",
    "ax.set_xticklabels( [ 'Dealer', 'Order\\nSize', 'Competitive', 'Pickup' ] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The statement *data = pd.melt( df_south )* was used.  The *melt* method in the Pandas package stacks the columns in the DataFrame into a new DataFrame so that the column names of the original DataFrame become a single new variable in the new stacked DataFrame (with name *variable*) and the values in each column of the original DataFrame become the values in a single new variable in the stacked DataFrame (with name *value*).  For this example, there are four discounts, so four columns, in *df_south*.  These four are stacked to create, or are *melted* to create, a new DataFrame with two columns: *variable* and *value*.  The melting takes a DataFrame that is in *wide form* and converts it to one in *long form*.  The long form is needed for the boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the dealer discount tends to be the largest while the order discount has the most variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Check the distribution of the pocket price by marketing region, loyalty program membership, and buyer rating. What do you conclude?  A complete Data Dictionary is [here](#Appendix-Complete-Data-Dictionary).\n",
    "\n",
    "[See Solution](#Solution-III.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by region.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by loyalty program.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by buyer rating.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2 Look for Relationships in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Scatter plots are the workhorse of statistical displays because they allow you to see relationships -- sometimes.  Properly drawn, they can provide a wealth of insight into: \n",
    "\n",
    "> - relationships;\n",
    "- trends;\n",
    "- patterns; and\n",
    "> - anomalies\n",
    "\n",
    "of two continuous variables.  They can be supplemented with histograms on the margins to show distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '061' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '062' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1 Transformation for Better Interpretation\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Since one objective from the product manager is to estimate a price elasticity, you should graph unit sales and Pocket Price.  We noticed earlier that unit sales were right skewed but that using a log transform shifted the distribution to a more normal one.  We should take the log of pocket price as well as unit sales.  This is a very common transformation in empirical demand analysis because the slope of a line relating sales to price is the elasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( \"063\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Transform unit sales and pocket price\n",
    "##\n",
    "df[ 'log_Pprice' ] = np.log1p( df.Pprice )\n",
    "df[ 'log_Usales' ] = np.log1p( df.Usales )\n",
    "##\n",
    "## Display the unlogged and logged data\n",
    "##\n",
    "lst = [ 'Pprice', 'log_Pprice', 'Usales', 'log_Usales' ]\n",
    "df[ lst ].head().style.set_caption( 'Sales and Price Data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the logged data\n",
    "## Use the Seaborn \"relplot\" function\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', xlabel = 'Log Pocket Price', \n",
    "       ylabel = 'Log Unit Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "A negative relationship is evident -- as it should be.  But the large number of plot points makes it slightly difficult to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.2 Enhancing the Scatter Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Replot the logged data with a regression line added. \n",
    "## Use the Seaborn \"regplot\" function.\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "## Note: \n",
    "##   The plot element colors can be set:\n",
    "##     b:blue, g:green, r:red, c:cyan,\n",
    "##     m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                 scatter_kws = { 'color':'black' },\n",
    "                 line_kws ={ 'color':'yellow' } )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Log Pocket Price', ylabel = 'Log Unit Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The Seaborn *regplot* function is used to add a regression line to the scatter plot.  To help distinguish between plotting points and the regression line, the *scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"yellow\"}* arguments are used.  The points are specified as black and the line as yellow.  The default is for both to be the same color.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The regression line shows a negative relationship between price and sales.\n",
    "\n",
    "**_Appendix_**\n",
    "\n",
    "Some additional graphs are in the Appendix [here](#Additional-Scatter-Plot-Methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3 Working with *Large-N* Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The scatter plots are dense, making it difficult to see patterns. Options are to use a:\n",
    "\n",
    "> 1. random sample;\n",
    "> 2. contour plot;\n",
    "> 3. hex bin plot; or\n",
    "> 4. Lowess smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3.1 Random Sampling\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '065' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Draw a random sample of size n = 500\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = df.sample( n = 500, random_state = 1234, replace = False )\n",
    "##\n",
    "## Plot the data using the random sample\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = smpl )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nRandom Sample\\nn = 500', \n",
    "       ylabel = 'Log Unit Sales', xlabel = 'Log Pocket Price' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The Pandas DataFrame method *sample* is used to draw a random sample without replacement of size $n = 500$.  The random seed is set at 1234 so the same sample would be drawn each time the cell is run.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The negative relationship between unit sales and price is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a random sample of $n = 1000$ and plot unit sales vs. total discounts (*Tdics*).  What do you conclude?\n",
    "\n",
    "[See Solution](#Solution-III.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3.2 Contour Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '066' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '067' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contour plot with marginal distributions\n",
    "## Sample\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', kind = 'kde', data = smpl );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Seaborn's *jointplot* is used.  The *kind = kde* argument is used for a *kernel density plot* which is the contours.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The dark spot in the middle shows the concentration of the data points.  The negative relationship between sales and price is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3.3 Hex Bin Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '068' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Hex binning\n",
    "## Sample\n",
    "##\n",
    "## Note: A white background is best for this \n",
    "## Note: \n",
    "##   The plot element colors can be set: \n",
    "##     b:blue, g:green, r:red, c:cyan,\n",
    "##     m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "with sns.axes_style( 'white' ):\n",
    "    ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', \n",
    "                       kind = 'hex', color = 'k', data = smpl );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3.4 Lowess Curve\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "An alternative is to fit a *Lowess Smooth* to the data.  *Lowess* stands for *Locally Weighted Scatterplot Smooth*.  It is a regression fit and approaches the *OLS* line for very smooth fits.  See <a href=\"https://en.wikipedia.org/wiki/Local regression\" target=\"_parent\">here</a> for a description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fit a Lowess Smooth with the scatter turned off\n",
    "## Use the sample data\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', lowess = True, scatter = False, data = smpl )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nRandom Sample\\nn = 500\\nWith Lowess Smooth', \n",
    "       ylabel = 'Log Unit Sales', xlabel = 'Log Pocket Price' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Study the relationship between Total Discount (*Tdisc*) and the pocket price (*Pprice*).  Use a random sample of $n = 200$, a Lowess smooth, and omit the scatter points.  Let the pocket price be on the vertical axis.  What can you conclude?\n",
    "\n",
    "[See Solution](#Solution-III.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3 Look for Trends in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Trends are identified using line graphs, usually with time on the X-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '071' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the date indicator and the Dealer discount\n",
    "##\n",
    "lst = [ 'Tdate', 'Ddisc' ]\n",
    "data = df[ lst ].copy()\n",
    "##\n",
    "## Reset the index to the date\n",
    "##\n",
    "data.set_index( 'Tdate', inplace = True )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The subset DataFrame containing *Tdate* and *Ddisc*, *data*, is reindexed using *Tdate*.  *Tdate* was converted to a *DateTime* variable when the orders data were originally imported. \n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The data for *Ddisc* are by year-month-day.  Notice that there are missing values indicated by *NaN*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Group the data by months and calculate the \n",
    "## mean discount for each month.\n",
    "##\n",
    "grp = data.resample( 'M' ).mean()\n",
    "grp.head().style.set_caption( 'Grouped Data' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *data* DataFrame is *resampled* to monthly data, the resampling using the mean of values in each month.  Basically, *resample* aggrgegates the data by the datetime index.  See <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" target=\"_parent\">here</a> for documentation on *resample*.\n",
    "\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Each value is the mean of values for the indicated month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Pandas' plot function.\n",
    "## It automatically uses the time index for the X-axis.\n",
    "##\n",
    "ax = grp.plot( y = 'Ddisc', legend = False )\n",
    "ax.set( title = 'Dealer Discount\\nMonthly', ylabel = 'Dealer Discount', xlabel = 'Months' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Pandas does not connect points if a point is missing. Pandas gives a better representation and is better with time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4 Look for Patterns in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Patterns are identified using a variety of visual displays.  So all the graph types discussed will help identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '074' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5 Look for Anomalies in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '076' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Categorical plot: boxplot variant\n",
    "##\n",
    "ax = sns.catplot( 'Tdisc', kind = 'box', orient = 'v', data = df_orders )\n",
    "ax.set( title = 'Total Discount\\nOutliers', ylabel = 'Total Discount', xlabel = '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There are some clear outliers:\n",
    "\n",
    "> 1. A number of points are very low.\n",
    "> 2. Only one or two points are very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.6 What is Next?\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "In Lesson IV, I will show you how to build three predictive models:\n",
    "\n",
    "> 1. *OLS*;\n",
    "> 2. Logit; and\n",
    "> 3. Decision trees.\n",
    "\n",
    "I'll discuss these in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( \"078\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson IV Predictive Modeling: Introduction to Machine Learning\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '080' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1 Comparing and Contrasting Prediction and Forecasting\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '082' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2 Steps for Predictive Modeling\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '084' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2.1 Steps for Predictive Modeling: Train/Test Split Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '086' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '087' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '088' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are split into two parts using *sklearn*.  Each part has a *X* variable array and a *y* vector (The upper and lower cases are conventional).  The *X* array is a Pandas DataFrame of the *X* variables.  The *y* vector is a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Notice the cases for the variable names.\n",
    "##\n",
    "y = df[ 'Usales' ]\n",
    "##\n",
    "lst = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region', 'buyerRating' ]\n",
    "X = df[ lst ]\n",
    "##\n",
    "## Split the data.  The default is 3/4 train.\n",
    "## Note: the train_test_split function was loaded in the packages section\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The dependent and independent variables need to be separated from the main DataFrame before the train/test split can be done.  The index from the main DataFrame is preserved.  The first three lines of code do this.  The *train_test_split* function randomly divides the data, keeping the indexes aligned.  The *random_state = 42* argument sets the random seed.  Four data sets are returned which are (in order): \n",
    "\n",
    "> *X_train*\n",
    ">\n",
    "> *X_test*\n",
    ">\n",
    "> *y_train*\n",
    ">\n",
    "> *y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print( 'Sample sizes:\\n\\tX: {}, y: {}'.format( X_train.shape[ 0 ], y_train.shape[ 0 ] ) )\n",
    "print( '\\nTraining X Data:\\n{}'.format( X_train.head() ) )\n",
    "print( '\\nTraining y Data:\\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the sample sizes for the training and testing data sets are the same.  Also note the indexes for the training and testing data sets.  These are the same as the main DataFrame, *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: Usales\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n{}'.format( train.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *X* and *Y* training data sets are merged on the indexes.  Recall that the indexes were preserved when the *X*  and *y* data sets were created.  This is why the indexes were the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the X and y testing data sets for predicting.\n",
    "\n",
    "[See Solution](#Solution-IV.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add log Usales and log Pprice to the training data\n",
    "## The log is based on the Numpy function log1p\n",
    "## Note: log1p( x ) = log( 1 + x )\n",
    "##\n",
    "train[ 'log_Usales' ] = np.log1p( train.Usales )\n",
    "train[ 'log_Pprice' ] = np.log1p( train.Pprice )\n",
    "print( 'Training Data Set:\\n{}'.format( train.head() ) )\n",
    "print( '\\nTraining Data Set Shape:\\n{}'.format( train.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Logged terms are added because the Data Visualization showed that logs induce normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Add log Usales and log Pprice to the testing data set.\n",
    "\n",
    "[See Solution](#Solution-IV.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2.2 Steps for Predictive Modeling: Train a Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "I will cover three predictive models:\n",
    "\n",
    "> 1. *OLS*\n",
    "> 2. Logit\n",
    "> 3. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '091' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '092' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '093' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '094' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '096' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '097' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case I Continuous Dependent Variable: *OLS* Regression\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Model unit sales as a function of the pocket price to get a price elasticity.  Recall that you are using log terms and that the estimated coefficient for log price is the elasticity.\n",
    "\n",
    "**Recommendation**:  Use formulas to specify the model.  You need the *statsmodels.formula* api for this.  Se the package loading section [here](#II.2.2-Load-Packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '099' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '100' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "## The formula uses a “~” to separate the left-hand side from the right-hand side\n",
    "## of a model and a “+” to add columns to the right-hand side.  A “-” sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_Usales ~ log_Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model <===\n",
    "##\n",
    "mod = smf.ols( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##      Recommendation: number your fitted models\n",
    "##\n",
    "reg01 = mod.fit() \n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( reg01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The modeling follows four steps as shown above.  Regardless of the software you use, these same four steps are followed.  Some software combine them, others require explicit statement.  This is what statsmodels requires.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The price elasticity is the coefficient for the logged price variable (i.e., log_Pprice): -1.7.  If price falls by 1\\%, unit sales rise by 1.7%.  This indicates that blinds are highly elastic.  This should be expected since furniture is a competitive business and blinds are very competitive.  Revenue will also change.  If price falls, revenue increases.  The amount revenue increases (in percentage terms) is given by $1 + elasticity$.  So for a 1% fall in price, revenue will rise 0.7% (= $1 + [-1.7]$). \n",
    "\n",
    "The discounts and regions seem to have no effect, but this can be tested as shown below.  Also note that the $R^2 = 0.20$ which is very low.  \n",
    "\n",
    "The Jarque-Bera Test is a test for normality of the disturbance term.  It is a test of the \"goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. $\\ldots$ The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.  $\\ldots$ If it is far from zero, it signals the data do not have a normal distribution.\"  So the Null Hypothesis is $H_O: Normality$.  (Source: <a href=\"https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test\" target=\"_parent\">see here</a>)  In this case, the Null is rejected.  The Omnibus Test is an alternative test of normality with the same Null.  It also indicates that the Null must be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Estimate a new OLS model by adding the buyer rating to the above model. Name your model *regE01*.  Interpret your results.  Is the buyer rating important for sales?\n",
    "\n",
    "**Hint**: Buyer rating is categorical so you have to create dummies for the rating.\n",
    "\n",
    "[See Solution](#Solution-IV.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case I Analyze the Results\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Quantities of interest can be extracted directly from the fitted model. Type *dir(results)* for a full list.\n",
    "\n",
    "Since the product manager wanted to know about a region effect, you should do an F-test of all the coefficients for the regions to determine if they are all zero, meaning that the dummies as a group do nothing.  This is a <u>joint</u> test of significance.  The test statistic is:\n",
    "\n",
    "> $F_C = \\dfrac{\\left(SSR_U - SSR_R\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)} = \\dfrac{\\left(SSE_R - SSE_U\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)}$\n",
    "\n",
    "where \"U\" indicates the *unrestricted* or *full* model with the Region dummies and \"R\" indicates the *restricted* model without the Region dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify the joint (Null) hypothesis that the regions are the same;\n",
    "## i.e., there is no region effect.\n",
    "##\n",
    "hypothesis = ' ( C(Region)[T.Northeast] = 0, C(Region)[T.South] = 0, C(Region)[T.West] = 0 ) '\n",
    "print( 'Null Hypothesis:\\n\\t{}'.format( hypothesis ) )\n",
    "print( '\\nAlternative Hypothesis:\\n\\tAt least one is not zero')\n",
    "##\n",
    "## Run an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "##\n",
    "## Retrieve the p-value\n",
    "##\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "##\n",
    "## Print results\n",
    "##\n",
    "print( '\\np-value for F-Test: {}'.format( pval ) )\n",
    "if pval < 0.05:\n",
    "    print( '\\nSignificant so reject H0' )\n",
    "else:\n",
    "    print( '\\nInsignificant so do not reject H0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that there are only three regions specified even though there are four: one is omitted as the base.  Also notice that the three hypotheses are specified as *C(Region)[T.XX] = 0* where *XX* is the region name.  The *T* stands for *Treatment* which is the *R* and *Python* term for a dummy variable encoding of a categorical variable.  There are other forms of encoding.  See [here](https://www.statsmodels.org/dev/examples/notebooks/generated/contrasts.html) for documentation on encoding a categorical variable in Statsmodels.\n",
    "\n",
    "**_Output Interpretation_**\n",
    "\n",
    "There are several returned values for the F-test.  Only the p-value is important.\n",
    "\n",
    "**_Interpetation_**\n",
    "\n",
    "The Null Hypothesis is that there is no region effect.  The p-value is 0.32 so the Null Hypothesis is not rejected: there is no Region effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Test the Null Hypothesis that all the buyer rating estimated parameters are zero.  That is, there is no difference among the ratings.\n",
    "\n",
    "[See Solution](#Solution-IV.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case I Predict with the Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Predict unit sales.  Recognize that sales are in (natural) log terms.  You will convert back to unit sales in \"normal\" terms later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate predicted log of unit sales, the dependent variable.\n",
    "##\n",
    "## Note: the inverse of the log is needed; use np.expm1( x )\n",
    "## since log1p was used: np.expm1 = exp(x) - 1.\n",
    "##\n",
    "log_pred = reg01.predict( test )  ## test is the testing data from Exercises IV.1 and IV.2\n",
    "y_pred = np.expm1( log_pred )\n",
    "##\n",
    "##\n",
    "## Combine into one temporary DataFrame for convenience\n",
    "##\n",
    "data = pd.DataFrame( { 'y_test':y_test, 'y_logPred':log_pred, 'y_pred':y_pred } )\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the sklearn metrics function *r2_score* to check the fit of actual vs. predicted values.  From the sklearn User Guide:\n",
    "\n",
    "> \"*The r2_score function computes R², the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display the r2 score.  But first drop any NaN data.\n",
    "##\n",
    "data.dropna( inplace = True )\n",
    "print( 'r2 Score:\\n{}'.format( round( r2_score( data.y_test, data.y_pred ), 3 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also graph the actual vs predicted values.  Sometimes, however, the number of data points is too large to plot so a random sample may be needed.  This is our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Draw a random sample of 500 observations without replacement\n",
    "## from the tmp DataFrame.\n",
    "##\n",
    "smpl = data.sample( n = 500, random_state = 1 )\n",
    "##\n",
    "## Plot the data\n",
    "##\n",
    "ax = sns.regplot( x = 'y_test', y = 'y_pred', scatter = True, data = smpl )\n",
    "ax.set( title = 'Actual vs Predicted Units Sales\\nRandom Sample of 500', \n",
    "       ylabel = 'Predicted Sales', xlabel = 'Actual Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unit sales for different settings of the variables.  This is *scenario* or *what-if* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dictionary (i.e, a dict)\n",
    "##\n",
    "data = {\n",
    "         'Pprice': [ 2.50 ],\n",
    "         'Ddisc': [ 0.03 ],\n",
    "         'Odisc': [ 0.05 ],\n",
    "         'Cdisc': [ 0.03 ],\n",
    "         'Pdisc': [ 0.03 ],\n",
    "         'Region': [ 'West' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_Pprice',\n",
    "                value = np.log1p( scenario.Pprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = reg01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.5\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a scenario with the following settings for your model with buyer Rating (regE01):\n",
    "\n",
    "> - 'Pprice': [ 2.50 ]\n",
    "> - 'Ddisc': [ 0.03 ]\n",
    "> - 'Odisc': [ 0.03 ]\n",
    "> - 'Cdisc': [ 0.03 ]\n",
    "> - 'Pdisc': [ 0.03 ]\n",
    "> - 'buyerRating': [ 'Poor' ]\n",
    "> - 'Region': [ 'South' ]\n",
    "\n",
    "[See Solution](#Solution-IV.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case II Binary Dependent Variable: Logistic Regression\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '103' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '104' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '105' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Create Your Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is part of the DataFrame.  Satisfaction is measured on a five-point scale: *1 = Not at All Satisfied*, *5 = Very Satisfied*.  \n",
    "\n",
    "First, look at the frquency count of satisfaction.  But, there is a problem: you cannot use the same data as before since satisfaction is by customer and the data used so far are by transaction.  The satisfaction rating is in the customer DataFrame.  Region, which will be included in the model, is in the marketing DataFrame, *df_marketing*.  You need to first find the mean price and mean discounts by customer from the transactions DataFrame and then merge this new DataFrame with the customer DataFrame.  So, there are several steps:\n",
    "\n",
    "> 1. Extract the pocket price and discounts -- include the *CID*\n",
    "2. Group by the CID and calculate the means by *CID*\n",
    "3. Merge with the customer DataFrame\n",
    "4. Recode the satisfaction values in the merged file so that 1 is the top-two values (called *top-two box* or *T2B*) and 0 is all other values.  The *T2B* is *Very Satisfied*.\n",
    "> 5. Train a model with *T2B* satisfaction as a function of the pocket price, discounts, and Region.\n",
    "\n",
    "The Customer Satisfaction variable is documented in the complete Data Disctionary [here](#Appendix-Complete-Data-Dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## ===> Step 1: Extract the pocket price and discounts -- include the CID <===\n",
    "##\n",
    "lst = [ 'CID', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "data = df[ lst ].copy()\n",
    "##\n",
    "## Set the index to the CID\n",
    "##\n",
    "data.set_index( 'CID', inplace = True )\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = data.shape[ 0 ], cols = data.shape[ 1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 2: Group by CID and calculate the means by CID <===\n",
    "##\n",
    "grp = data.groupby( 'CID' ).mean()\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = grp.shape[ 0 ], cols = grp.shape[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Look at the head of grp\n",
    "##\n",
    "grp.head().style.set_caption( 'Mean Price and Discounts' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 3: Merge with the customer and marketing DataFrames <===\n",
    "##              Merge on the CID\n",
    "##\n",
    "df_sat = pd.merge( pd.merge( grp, df_cust, on = 'CID' ), df_marketing, on = 'CID' )\n",
    "##\n",
    "## Alternative merge:\n",
    "## df_sat = grp.merge( df_cust, on = 'CID' ).merge( df_marketing, on = 'CID' )\n",
    "## \n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_sat.shape[ 0 ], cols = df_sat.shape[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the columns\n",
    "##\n",
    "df_sat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do a quick check of the satisfaction distribution.\n",
    "##\n",
    "## Use the DataFrame's value_counts() method. Sort by the\n",
    "## scale values 1 - 5.\n",
    "##\n",
    "data = df_sat.buyerSatisfaction.value_counts( normalize = True, sort = False )\n",
    "pd.DataFrame( data ).style.set_caption( 'Buyer Satisfaction' ).\\\n",
    "bar( align = 'mid', color = 'red' ).format( '{:.1%}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *normalize = True* argument divides the count for each satisfaction level by the sum of the counts to give the proportion of *CID*s at each level.  These proportions should sum to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 4: Recode the scale values so that 1 is the top-two values <===\n",
    "## (called \"top-two box\" or \"T2B\") and 0 is all other values.  \n",
    "## The \"T2B\" is \"Very Satisfied\".\n",
    "##\n",
    "## Define a lambda function for the recoding\n",
    "##\n",
    "df_sat[ 'sat_t2b' ] = df_sat.buyerSatisfaction.apply( lambda x: 1 if ( x >= 4 ) else 0 )\n",
    "##\n",
    "data = df_sat[ 'sat_t2b' ].value_counts( normalize = True ).round( 3 )\n",
    "data = pd.DataFrame( data )\n",
    "data.rename( index = { 1:'T2B', 0:'B3B' }, inplace = True )\n",
    "pd.DataFrame( data ).style.set_caption( 'Buyer Satisfaction' ).\\\n",
    "bar( align = 'mid', color = 'red' ).format( '{:.1%}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *T2B* satisfaction as a function of the pocket price, discounts, and Region.  First, create training and testing DataFrames as before but with *sat_t2b* as the *y* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 5: Split the Data. <===\n",
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df_sat[ 'sat_t2b' ]\n",
    "lst = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region' ]\n",
    "X = df_sat[ lst ]\n",
    "##\n",
    "## Split the data.  The default is 1/3 testing.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.33, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data: training data\n",
    "##\n",
    "print(\"Sample sizes: \\nX Training: {}, y Training: {}\\n\".format( X_train.shape[ 0 ], y_train.shape[ 0 ] ) )\n",
    "print( '\\nX Training Data: \\n{}'.format( X_train.head() ) )\n",
    "print( '\\ny Training Data: \\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data: testing data\n",
    "##\n",
    "print(\"Sample sizes: \\nX Testing: {}, y Testing: {}\\n\".format( X_test.shape[ 0 ], y_test.shape[ 0 ] ) )\n",
    "print( '\\nX Testing Data: \\n{}'.format( X_test.head() ) )\n",
    "print( '\\ny Testing Data: \\n{}'.format( y_test.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the respective sample sizes sum to 779 (= 521 Train + 258 Test) which is the total unique *CID*s from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "train.head().style.set_caption( 'Training Data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of the merged training data\n",
    "##\n",
    "print( 'Training Sample size:\\n {}'.format( train.shape[ 0 ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "test.head().style.set_caption( 'Testing Data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of the merged testing data\n",
    "##\n",
    "print( 'Testing Sample size:\\n{}'.format( test.shape[ 0 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Train a Model\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Train a logit model\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'sat_t2b ~ Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## ===> Step 2: Instantiate the logit model <===\n",
    "##\n",
    "mod = smf.logit( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Predict with the Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The prediction process is the same as discussed for *Case I* above, but now you can compare actuals and predicted using a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Make predictions\n",
    "##\n",
    "predictions = logit01.predict( test )  ## test is the testing dataset\n",
    "predictions_nominal = [ 0 if x < 0.5 else 1 for x in predictions ]\n",
    "rpt = classification_report( y_test, predictions_nominal, digits = 3 )\n",
    "print( 'Logit Model Classification Report:\\n{}'.format( rpt ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "To quote from [here](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8):\n",
    "\n",
    "> *The precision is the ratio $tp/(tp + fp)$ where $tp$ is the number of true positives and $fp$ the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.*\n",
    ">\n",
    "> *The recall is the ratio $tp/(tp + fn)$ where $tp$ is the number of true positives and $fn$ the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.*\n",
    ">\n",
    "> *The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.*\n",
    ">\n",
    "> *The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.*\n",
    ">\n",
    "> *The support is the number of occurrences of each class in y_test.*\n",
    "\n",
    "Also see [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal ).ravel()\n",
    "##\n",
    "## zip the variable names and the confusion\n",
    "##\n",
    "lbl = [ 'True Negative', 'False Positive', 'False Negative', 'True Positive' ]\n",
    "##\n",
    "## Display the confusion matrix in a DataFrame\n",
    "##\n",
    "df_confusion = pd.DataFrame( list( zip( lbl, cm ) ), columns = [ 'Label', 'Value' ] )\n",
    "df_confusion[ 'Percent' ] = round( ( df_confusion.Value/df_confusion.Value.sum() )*100, 1 )\n",
    "df_confusion.style.set_caption( 'Confusion Matrix' ).bar( align = 'mid', color = 'red' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the confusion values\n",
    "##\n",
    "ax = sns.barplot( y = df_confusion.Label, x = df_confusion.Percent )\n",
    "ax.set( title = 'Percent of Sample\\nby Confusion Labels',\n",
    "        xlabel = 'Percent Confusion', ylabel = '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There were:\n",
    "\n",
    "> 1 true negatives\n",
    ">\n",
    "> 81 false positives\n",
    ">\n",
    "> 3 false negatives\n",
    ">\n",
    "> 173 true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Satisfied', 'Satisfied']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal )\n",
    "data = pd.DataFrame( data = cm, index = lbl, columns = lbl )\n",
    "print( 'Confusion Matrix: \\n{}'.format( data ) )\n",
    "##\n",
    "## Plot the confusion matrix\n",
    "##\n",
    "sns.set( font_scale = 1.4 )   #for label size\n",
    "##\n",
    "ax = sns.heatmap( cm/cm.sum(), annot = True, annot_kws = { \"size\": 16 } )  # font size\n",
    "ax.set( title = 'Confusion Matrix for the Classifier', xlabel = 'Predicted',\n",
    "       ylabel = 'True' )\n",
    "ax.set_xticklabels( lbl )\n",
    "ax.set_yticklabels( lbl );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "67% of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case III Constants: Decision Trees\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Decision Trees can handle continuous or discrete dependent variables.  They are an alternative to *OLS* and logistic regression: you don't have to specify a \"model\" *per se*.  They also have the advantage that a visual display, a *tree*, is produced which is easier for management and clients to understand than complex regression output and statistics.  You will only look at a discrete case; a continuous case is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '108' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '109' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '110' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '111' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '112' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '113' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Instantiate LabelEncoder for Region\n",
    "##\n",
    "labelencoder = LabelEncoder()\n",
    "##\n",
    "## Convert \"Region\" to integers: the decision tree must have all numerics\n",
    "## Note 1: use the LabelEncoder function for this\n",
    "## Note 2: \"Region\" will be encoded in alphanumeric order:\n",
    "##\n",
    "##          0: Midwest\n",
    "##          1: Northeast\n",
    "##          2: South\n",
    "##          3: West\n",
    "##\n",
    "## Encode the Region labels in the training and testing data sets using Label Encoder\n",
    "## First copy the DataFrames\n",
    "##\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "##\n",
    "## Label encode\n",
    "##\n",
    "print( '\\nTraining Data Before Encoding Region:\\n \\n{}'.format( X_train_copy.head() ) )\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_train_copy[ 'Region_encoded' ] = labelencoder.fit_transform( X_train_copy[ 'Region' ] )\n",
    "print( '\\nTraining Data After Encoding Region:\\n{}'.format( X_train_copy.head() ) )\n",
    "##\n",
    "## drop Region since Region_encoded will be used\n",
    "##\n",
    "X_train_copy = X_train_copy.drop( columns = ['Region'] )\n",
    "print( '\\nTraining Data After Dropping Region:\\n{}'.format( X_train_copy.head() ) )\n",
    "##\n",
    "## Apply the label encoder to the test DataFrame\n",
    "##\n",
    "print( '\\n\\n' )\n",
    "print( '\\nTesting Data Before Encoding Region:\\n \\n{}'.format( X_test_copy.head() ) )\n",
    "X_test_copy[ 'Region_encoded' ] = labelencoder.fit_transform( X_test_copy[ 'Region' ] )\n",
    "print( '\\nTesting Data After Encoding Region:\\n{}'.format( X_test_copy.head() ) )\n",
    "X_test_copy = X_test_copy.drop( columns = [ 'Region' ] )\n",
    "print( '\\nTesting Data After Dropping Region:\\n{}'.format( X_test_copy.head() ) )\n",
    "##\n",
    "## Instantiate the tree\n",
    "## Specify:\n",
    "##    - Depth: 3 levels\n",
    "##    - Minimum sample size for a leaf: 5\n",
    "##\n",
    "dtree = tree.DecisionTreeClassifier( random_state = 0, max_depth = 3, \n",
    "                                    min_samples_leaf = 5 )\n",
    "##\n",
    "## Fit the tree\n",
    "##\n",
    "dtree.fit( X_train_copy, y_train_copy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case III Check Model Accuracy\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Check accuracy scores\n",
    "##\n",
    "print( \"Accuracy on training data: {:.3f}\".format( dtree.score( X_train_copy, y_train_copy ) ) )\n",
    "print( \"Accuracy on testing data: {:.3f}\".format( dtree.score( X_test_copy, y_test_copy ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are good scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case III Display the Tree\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Displaying a tree is a slight challenge!\n",
    "## There are four steps:\n",
    "##\n",
    "## ===> Step 1: Create a placeholder for all the plotting points.\n",
    "##\n",
    "dot_data = StringIO()\n",
    "##\n",
    "## ===> Step 2: Extract the feature names for labels models\n",
    "##\n",
    "feature_names = [ i for i in X_train.columns ]\n",
    "##\n",
    "## ===> Step 3: Export the plotting data to the placeholder\n",
    "##\n",
    "export_graphviz( dtree, out_file = dot_data,  \n",
    "                filled = True, rounded = True,\n",
    "                special_characters = True,\n",
    "                class_names = [ 'Not Satisfied', 'Satisfied' ],\n",
    "                feature_names = feature_names ,\n",
    "                proportion  = True\n",
    "               )\n",
    "##\n",
    "## ===> Step 4: Create the display\n",
    "##\n",
    "graph = pydotplus.graph_from_dot_data( dot_data.getvalue() )  \n",
    "Image( graph.create_png() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "For the right node on the third level, \"$Region \\le 2.5$\" is interpreted as *Region* having a value less than or equal to 2.5.  Since $Region = 0$, $Region = 1$, and $Region = 2$ meet this criteria and $Region = 3$ is the West, then if *Region* is Midwest/Northeast/South, go to the left; otherwise, go to the right for the West. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExerciseI V.6\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Interpret the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson V Summary and Wrap-up\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '116' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Information\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "If you have any questions after this course, please do not hesitate to contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '118' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix contains material extra to this lesson, material that you may want to review to solidify your understanding and knowledge about working with Python and Pandas.\n",
    "\n",
    "This Appendix covers:\n",
    "\n",
    "1. Jupyter Notebook: Overview;\n",
    "2. Importing data into Pandas;\n",
    "3. Checking your Data;\n",
    "4. Manipulating columns of a Pandas DataFrame;\n",
    "5. Correlation Analysis;\n",
    "6. Data Visualization; and \n",
    "7. OLS Modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.1 Jupyter Notebooks: Overview\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Jupyter is a sophisticated programming tool sometimes called an *ecosystem*.  It is an ecosystem because it will handle a number of programming languages, Python being one of them.  The langauges are called _kernels_.  Other kernels are R, Julia, Fortran to mention a few.  There are over 100 kernels supported by Jupyter.  Jupyter was originally created to handle Julia, Python, and R.  In fact, the name \"Jupyter\" is a contraction for **JU**lia, **PYT**hon, and **R**.\n",
    "\n",
    "The paradigm for Jupyter is a lab notebook used in the physical sciences for laboratory experiment documentation.  The Jupyter notebook consists of cells where text and programming code are entered and executed or *run*.  There are two basic cells:\n",
    "\n",
    "1. Code cell; and \n",
    "2. Markdown cell.\n",
    "\n",
    "A code cell is where programming code is entered and executed.  The results are displayed in another cell that appears immediately below a code cell.  To organize the code and result, the pair of cells are labeled with a marker for the code and the result.  The code marker is In[] and the result marker is Out[].  Inside the square brackets are numbers representing the sequence in which code is executed.  The numbers will vary, and even be out of order, if a code cell is executed several times but a following code cell is not reexecuted.  The sequencing numbers can be reset by rerunning the kernel.  This is done by clicking on *Kernel* on the main toolbar and selecting *Restart & Run All*.  \n",
    "\n",
    "The markdown cell is where documentation is entered.  This cell, in fact, is a markdown cell.  It is that you use as many markdown cells as possible to document your work.  You will see many examples of this below.<br>\n",
    "\n",
    "A code cell is the default.  You make a code cell a markdown cell by using the drop-down menu list on the main toolbar.  Of course, you can change a markdown cell to a code cell using the same drop-down list.  You could also use keyboard short-cuts.  See *Help/Keyboard Shortcuts* on the main menu bar. \n",
    "\n",
    "You can easily insert/delete cells and move them from one location in your notebook to another.  To insert a cell above the current cell, select the cell as the base for the insert and click on the colored bar on the left until it turns blue.  Then type *A* to insert a cell above the current cell.  Type *B* to insert one below it.  Type *DD* (two *D*'s) to delete the current cell.  To move a cell, select the cell you want to move, click the colored bar on the left, and use the up-arrow and down-arrow icons on the main menu toolbar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.2 Different Ways to Import Data Into Pandas\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You could also define the path without the file name if you plan to use multiple files from the same directory.  You just have to define the general path and then concatenate the file name.  Here is an example.  Note:\n",
    "\n",
    "1. the \"+\" which instructs Pandas to add or concatenate the two strings;\n",
    "2. the quotes around the file name; and\n",
    "3. the forward slash as the last symbol in the path definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and concatenate the file name.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../Data/furniture/final data files/'\n",
    "## df_orders = pd.read_csv( path + 'orders.csv' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following form if your data are in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do not specify a path if the CSV file is in the same directory\n",
    "## as your notebook.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## df_orders = pd.read_csv( 'orders.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are in an Excel file, use *pd.read_excel( filename, 'sheetname' )*.  The sheet name could be a character string or a sheet number as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: Import an Excel file\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../Data/furniture/final data files/'\n",
    "## df_orders = pd.read_excel( path + 'orders.xlsx', 'furniture' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.3 Some Additional Information on Checking Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Look at:\n",
    "\n",
    "1. the head of your data;\n",
    "2. the shape of your DataFrame; \n",
    "3. a list of column names; and\n",
    "4. the missingness of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#1 Display the First Few Records of Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the head of the imported data\n",
    "## n = 5 is the default\n",
    "##\n",
    "df_orders.head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should immediately notice that several discounts have missing values indicated by *NaN* (*Not a Number*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#2 Check the Shape of Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: check the shape of the imported data\n",
    "## Note 1: the order of the returned shape is always #rows, #columns\n",
    "## Note 2: there is no () for this command because the shape\n",
    "## is a DataFrame attribute\n",
    "##\n",
    "print( \"Shape of the DataFrame: {}\".format( df_orders.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,270 rows or observations and 15 columns or variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#3 Check the Column Names in Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the column names\n",
    "##\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Remove white spaces in the column names\n",
    "## White spaces are not an issue here.  This is \n",
    "## just illustrative.\n",
    "##\n",
    "df_orders.columns = df_orders.columns.str.strip()\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#4 Check for Missing Data in Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use the DataFrame's information content.  The info()\n",
    "## method returns the number of non-missing rows for\n",
    "## each variable.  The numbers should be the same.\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above listing indicates that the four discounts each have missing values. \n",
    "\n",
    "You can count the number of missing values using the *isnull()* method and chaining the *sum()* function.  The *isnull()* method returns a Boolean variable so the *sum()* function just adds 0 and 1 values.  Use a nice print statement for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Sum the Boolean variable returned by isull()\n",
    "## Let us check the Order Discount (Odisc).\n",
    "##\n",
    "x = df_orders.Odisc.isnull().sum()\n",
    "print( 'Missing count for Odisc: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the proportion of each variable that is missing rather than the sum. Proportions are more meaningful. Use the *mean()* function for this.  Since *isnull()* returns a Boolean, the mean is just the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check for missing values only for the discounts.\n",
    "## Chain the mean() function to the isnull() method.\n",
    "## Note: Do this for first 20 records for illustration only.\n",
    "##\n",
    "lst = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ lst ].iloc[ :20 ].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a heatmap of missing data using the *isnull()* method on the entire DataFrame.  Use the transpose attribute, *T*, for a more readable chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Note: the \"cbar = False\" argument turns off the color bar\n",
    "## Note: Do this for first 20 records for illustration only\n",
    "##\n",
    "lst = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "ax = sns.heatmap( df_orders[ lst ].iloc[ :20, : ].isnull().T, cbar = False )\n",
    "ax.set( title = 'Heatmap of Missing Data' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.4 Miscellaneous Pandas DataFrame Column Manipulations\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting Columns\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can easily delete unwanted columns with the *drop* method.  You must specify if you want the DataFrame replaced or not; the default is to not replace in which case you must assign a new name to the modified DataFrame.  If you drop a column, it is good practice to not replace your DataFrame so that you preserve your original data.\n",
    "\n",
    "The *drop* method can be used to drop rows or columns, so you have to tell it which one.  This is done with the *axis* arugment.  The DataFrame, as a simple rectangular array, is said to have two *axes*: the row axis and the column axis.  Since in mathematics the size of a matrix is conventionally specified as $\\#row \\times \\#columns$ (rows always come before columns), the DataFrame axes are designated as 0 and 1 for rows and columns, respectively, since 0 comes before 1.  Specifying *axis = 0* in the *drop* method says to drop a row while specifying *axis = 1* says to drop a column. The default is *axis=0*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop a column and replace the DataFrame.  Notice that axis = 1 is used\n",
    "## to drop a column.\n",
    "## \n",
    "## Not Run\n",
    "##\n",
    "## df_orders.drop( 'obs', axis = 1, inplace = True )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.5 Correlation Analysis\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "There is a *correlation* method attached to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: display a correlation matrix of the discounts\n",
    "##\n",
    "lst = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "df[ lst ].corr().round( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The correlation matrix has a 1.0 in the cells along its main diagonal (the diagonal running from top left to bottom right).  The off-diagonal cells have the pair-wise correlations.  Notice that the correlation matrix is symmetric around the main diagonal: the top portion (called the *upper triangle*) matches the bottom portion (called the *lower triangle*).\n",
    "\n",
    "Round to three decimal places is usually sufficient.\n",
    "\n",
    "The correlations are all very low indicating that the discounts are not linearly associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *heatmap* is sometimes more effective for displaying a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: plot the correlation matrix as a heatmap\n",
    "##\n",
    "lst = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "cor = df[ lst ].corr()\n",
    "ax = sns.heatmap( cor )\n",
    "ax.set( title = 'Heatmap of the Correlation Matrix' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The cells along the main diagonal are all white which, by the color bar on the right, indicates they are all 1.0 as they should be.  All other cells are black indicating that the correlations are all 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix II.1 Data Visualization\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix contains material extra to this lesson, material that you may want to review to solidify your understanding and knowledge about working with Python, Pandas, and Seaborn for Data Visualization.\n",
    "\n",
    "This Appendix covers:\n",
    "\n",
    "1. Additional Histogram Methods;\n",
    "2. Additional Boxplot Methods;\n",
    "3. Additional Scatter Plot Methods; and\n",
    "4. Additional Time Series Plot Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Histogram Methods\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a *rug plot* to the bottom of the histogram to show each observation.  This is helpful to show where the data are for each bar in the histogram.  This, of course, is not practical for large data sets since the rug would just be a dense, black bar at the bottom of the graph.  \n",
    "<br>\n",
    "You can also remove the *KDE* curve for a better visualization of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Add a rug and remove the KDE\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales), kde = False, rug = True )\n",
    "ax.set( title = \"Unit Sales Distribution: Log Scale\", \n",
    "       xlabel = 'Unit Sales (Natural Log)', \n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display just the *KDE* curve for a cleaner view of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## KDE only\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales), hist = False )\n",
    "ax.set( title = \"Unit Sales Distribution: Log Scale\", \n",
    "       xlabel = 'Unit Sales (Natural Log)', \n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Boxplot Methods \n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the discounts by the customer loyalty status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions and Loyalty Program\n",
    "## members\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', hue = 'loyaltyProgram', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region \\n and \\n Loyalty Program',\n",
    "       ylabel = 'Total Discount' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Another view of total discount distribution by Regions and Loyalty Program\n",
    "## members\n",
    "##\n",
    "ax = sns.catplot( x = 'Tdisc', y = 'loyaltyProgram', row = 'Region',\n",
    "                kind = 'box', orient = 'h', height = 1.5, aspect = 4,\n",
    "                data = df )\n",
    "ax.set( xlabel = 'Total Discount', ylabel = 'Loyalty Program\\nMember' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be disturbing that the discounts are the same whether a customer is in the loyalty program or not.  Members should have bigger discounts.  What about how they are rated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions and buyer rating\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', hue = 'buyerRating', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region \\n and \\n Buyer Rating', \n",
    "       ylabel = 'Total Discount' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loyalty and good ratings are not rewarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Scatter Plot Methods \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix section covers:\n",
    "\n",
    "1. Categorical Variable\n",
    "2. Panel Plot\n",
    "3. Combining Scatter Plots and Histograms\n",
    "4. Pairwise Scatter Plots\n",
    "5. Contour Plots and Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Variable\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can add a third variable that is categorical to show relationships across groups.  This is done with a \"hue\" command which colors the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Loyalty Program membership\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'loyaltyProgram', \n",
    "                 data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Log Pocket Price', \n",
    "       ylabel = 'Log Unit Sales' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Region\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'Region', data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Pocket Price', ylabel = 'Unit Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Panel Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Loyalty Program membership\n",
    "## A less cluttered view with panels\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'loyaltyProgram', \n",
    "                 col = 'Region', col_wrap = 2,\n",
    "                 data = df )\n",
    "ax.set( xlabel = 'Pocket Price', ylabel = 'Unit Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice the gap between 17 and 19 in the Northeast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Scatter Plots and Histograms\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can combine scatter plots with histograms for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add histograms to the margins\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = df );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pairwise Scatter Plots\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can also plot multiple variables in pair-wise combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use the Seaborn pairwise function\n",
    "## Full sample\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "##\n",
    "## We know there are missing values for the discounts.\n",
    "## Missing values are not handled well with Seaborn histograms.\n",
    "## So drop all records with any missing data.\n",
    "##\n",
    "tmp = df[ x ].copy()\n",
    "tmp.dropna( inplace = True )\n",
    "sns.pairplot( tmp[ x ] );\n",
    "##\n",
    "## Warning -- this will take a few minutes\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Unfortunately, this particular plot is clearly not useful because the data set is large; we have a case of *Large-N*.  So how is this handled?  Try a random sample as in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Pairwise plot\n",
    "##\n",
    "## Random sample, n = 500 (previously drawn)\n",
    "##\n",
    "smpl = df.sample( n = 500, random_state = 1234 )\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "sns.pairplot( smpl[ x ] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "This is not much better.  Maybe a smaller sample will work.  You can try this on your own.  A contour or hex bin plot might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contour Plots with Density Functions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contour plot with marginal distributions\n",
    "## Random sample, n = 500\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = smpl, kind = 'kde' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "A different contour plot is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Hex binning\n",
    "##\n",
    "## Random sample, n = 500\n",
    "##\n",
    "## Note: A white background is best for this \n",
    "## Note: The plot element colors can be set:\n",
    "##   b:blue, g:green, r:red, c:cyan,\n",
    "##   m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "with sns.axes_style( 'white' ):\n",
    "    ax = sns.jointplot(x = 'log_Pprice', y = 'log_Usales', data = smpl, \n",
    "                       kind = 'hex', color = 'k' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add a regression line\n",
    "##\n",
    "## Full data sample\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "with sns.axes_style(\"white\"):\n",
    "    g = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                      kind = 'hex', color = 'k',\n",
    "                      joint_kws={'gridsize':40, 'bins':'log'} )\n",
    "    ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                     ax = g.ax_joint, scatter = False, color = \"yellow\" )\n",
    "    ax.set( xlabel = 'Log Pocket Price', ylabel = 'Log Unit Sales' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Time Series Plot Methods \n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Time series plot for Southern Region\n",
    "##\n",
    "lst = [ 'Tdate', 'Ddisc' ]\n",
    "data = df.loc[ df.Region == 'South', lst ]\n",
    "##\n",
    "## Reset the index to the date\n",
    "##\n",
    "data.Tdate = pd.to_datetime( data.Tdate )\n",
    "data.set_index( 'Tdate', inplace = True )\n",
    "grp = data.resample( 'M' ).mean()\n",
    "##\n",
    "## Create a Month variable from the index\n",
    "##\n",
    "grp[ 'x' ] = grp.index\n",
    "grp[ 'Month' ] = grp.x.dt.month\n",
    "print( grp.head() )\n",
    "##\n",
    "ax = grp.plot( y = 'Ddisc' , legend = False )\n",
    "ax.set( title = 'Dealer Discount\\nMonthly\\nSouthern Region', ylabel = 'Dealer Discount', xlabel = 'Months' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix III.1 Extra Material for Predictive Modeling\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check OLS Model for Multicollinearity\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Multicollinearity is a major issue with high-dimensional datasets.  A high level of multicollinearity can negatively impact estimation results.  It can be checked for using:\n",
    "\n",
    "1. a correlation matrix; or\n",
    "2. a variance inflation factor (VIF) measure.  A rule-of-thumb is that any $VIF > 10$ indicates a problem.\n",
    "\n",
    "This material assumes that the OLS model for Case I was estimated [here](#Case-I-Continuous-Dependent-Variable:-OLS-Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the correlation matrix\n",
    "## \n",
    "## Subset the design matrix to eliminate the first column of 1s\n",
    "## the iloc method says to find the location of columns based on \n",
    "## their integer locations (i.e., 0, 1, 2, etc.)\n",
    "## the term in brackets says to find all rows (the : ) and all \n",
    "## columns from the first to the end (1: )\n",
    "##\n",
    "data = reg01.model.data.orig_exog.iloc[ :, 1: ] \n",
    "corr_matrix = data.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Graph the correlation matrix\n",
    "##\n",
    "sns.heatmap( corr_matrix ).set_title( 'Heatmap of the Correlation Matrix' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## A fancy version of the heatmap\n",
    "## Based on: https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
    "##\n",
    "cmap = sns.diverging_palette( 5, 250, as_cmap = True )\n",
    "##\n",
    "corr_matrix.style.background_gradient( cmap, axis = 1 ).set_precision( 1 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate VIFs\n",
    "##\n",
    "## The VIFs are the diagonal elements of the inverted correlation\n",
    "## matrix of the independent variables.\n",
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s.\n",
    "## The iloc method says to find the location of columns based on their \n",
    "## integer locations (i.e., 0, 1, 2, etc.) the term in brackets says \n",
    "## to find all rows (the : ) and all columns from the first to the end (1: ).\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "data = reg01.model.data.orig_exog.iloc[ :, 1: ]\n",
    "corr_matrix = data.corr()\n",
    "##\n",
    "## Invert the correlation matrix and extract the main diaginal\n",
    "##\n",
    "vif = np.diag( np.linalg.inv( corr_matrix ) ) \n",
    "##\n",
    "## Zip the variable names and the VIFs\n",
    "##\n",
    "indepvars = [ i for i in data.columns ]\n",
    "xzip = zip( indepvars, vif ) \n",
    "##\n",
    "## Display the zip matrix.\n",
    "##\n",
    "pd.DataFrame( list( xzip ), columns = [ 'Variable', 'VIF' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *VIF*s are all below 10 so there is no problem.  $VIF > 10$ is a rule-of-thumb for indicating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case I Model Portfolio\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This is a nice way to summarize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import some packags\n",
    "##\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.api import anova_lm\n",
    "##\n",
    "## Create a variable to hold the model names; this is a list.\n",
    "## Note: the range() function specifies 1 - 2 but the \"2\" is\n",
    "## not included.\n",
    "##\n",
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "              'R2 Adjusted': lambda x: \"{:0.3f}\".format( x.rsquared_adj ),\n",
    "              'AIC': lambda x: \"{:0.2f}\".format( x.aic ),\n",
    "              'F': lambda x: \"{:0.2f}\".format( x.fvalue ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ reg01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case II Model Portfolio\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ logit01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Complete Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                              | Order Sys   | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Customer's State          | 50 US states + DC                      | Marketing Sys| State        |\n",
    "| ZIP Code                  | 5-digit US ZIP (postal) code           | Marketing Sys| ZIP          |\n",
    "| Marketing Region of Customer | Four US Census Regions              | Marketing Sys| Region       |\n",
    "| Member of Loyalty Program | Nominal: Yes/No                        | Marketing Sys| loyaltyProgram |\n",
    "| Rating of Customer        | Nominal: Poor/Good/Excellent           | Marketing Sys| buyerRating |\n",
    "| Cusomer Satisfaction Rating | 5 Point Likert Scale: 5 = Very Sat.  | Marketing Sys| buyerSatisfaction |\n",
    "| Total Discount            | \\%                                     | Calculated: Sum of Discounts | TDisc    |\n",
    "| Pocket Price              | \\$US                                   | Calculated: $Lprice \\times (1  - Tdisc)$| Pprice     |\n",
    "| Revenue                   | \\$US  | Calculated: $USales \\times Pprice$ | Rev |\n",
    "| Net Revenue               | \\$US  | Calculated: $(Usales - returnAmount) \\times Pprice$  | newRev |\n",
    "| Lost Revenue              | \\$US  | Calcualted: $Rev - netRev$    | lostRev |\n",
    "| Profit Contribution       | \\$US  | Calculated: $Rev - Mcost$    | Con |\n",
    "| Contribution Margin       | \\%    | Calculated: $\\dfrac{Con}{Rev}$ | CM |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.1\n",
    "\n",
    "Create a table in Markdown mode.\n",
    "\n",
    "[Return to Exercise II.1](#Exercise-II.1)\n",
    "\n",
    "| Variable                     | Values                              | Source       | Mnemonic          |\n",
    "|------------------------------|-------------------------------------|--------------|-------------------|\n",
    "| Customer ID                  | Nominal                             | Customer Sys | CID               | \n",
    "| Customer's State             | 50 US states + DC                   | Marketing Sys| State             |\n",
    "| ZIP Code                     | 5-digit US ZIP (postal) code        | Marketing Sys| ZIP               |\n",
    "| Marketing Region of Customer | Four US Census Regions              | Marketing Sys| Region            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.2\n",
    "\n",
    "[Return to Exercise II.2](#Exercise-II.2)\n",
    "\n",
    "The *Pocket Price* is the list price less total discounts or total leakages.  It is the amount the business \"pockets\" and is the amount the customer actually pays.  The pocket price formula is $Pprice = Lprice \\times (1  - Tdisc)$.  The *Tdisc* variable was created above.  Calculate the pocket price for the *df_orders* DataFrame and display the first five records for the list price and pocket price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )\n",
    "##\n",
    "## Display just the components of Pprice\n",
    "##\n",
    "lst = [ 'Lprice', 'Tdisc', 'Pprice' ]\n",
    "df_orders[ lst ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.3\n",
    "\n",
    "[Return to Exercise II.3](#Exercise-II.3)\n",
    "\n",
    "Calculate total revenue as $Rev = Usales \\times Pprice$ using the *df_orders* DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Multiply Unit Sales and Pocket Price\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice\n",
    "##\n",
    "## Create a list of unit sales, pocket price, and revenue\n",
    "##\n",
    "lst = [ 'Usales', 'Pprice', 'Rev' ]\n",
    "df_orders[ lst ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.4\n",
    "\n",
    "[Return to Exercise II.4](#Exercise-II.4)\n",
    "\n",
    "*Contribution* and *contribution margin* are two values financial analysts often examine.  Contribution is comparable to what economists call *profit* but is more restricted in that it just refers to a product without considering any fixed or overhead costs.  Contribution is $Con = Revenue - Material~Cost$ and contribution margin is $CM = \\dfrac{Con}{Revenue}$.  Calculate both quantities using the *df_orders* DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contribution: Subtract Material Cost (Mcost from the Data Dictionary) from Revenue\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost\n",
    "##\n",
    "## Contribution Margin: Divide Contribution by Revenue\n",
    "##\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev\n",
    "##\n",
    "## Create a list to display\n",
    "##\n",
    "lst = [ 'Usales', 'Pprice', 'Mcost', 'Rev', 'Con', 'CM' ]\n",
    "df_orders[ lst ].head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.5\n",
    "\n",
    "[Return to Exercise II.5](#Exercise-II.5)\n",
    "\n",
    "Some products are returned so another revenue number, *revenue net of returns*, is more meaningful and revealing for business decisions.  Net revenue is\n",
    "<br><br>\n",
    "$Net Revenue = (Unit Sales - Returns) \\times Pocket Price$.\n",
    "<br><br>\n",
    "Calculate net revenue and call it 'netRev'.  Also calculate the loss in revenue due to the returns.  The calculation is $lostRev = Rev - netRev$.  Use the *df_orders* DataFrame.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Net Revenue: Subtract the amount returned (returnAmount from the Data Dictionary) and \n",
    "## multiply by the Pocket Price\n",
    "##\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice\n",
    "##\n",
    "## Lost Revenue: Total revenue less the net revenue\n",
    "##\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev\n",
    "##\n",
    "## Create a list to display\n",
    "##\n",
    "lst = [ 'Rev', 'netRev', 'lostRev' ]\n",
    "df_orders[ lst ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.6\n",
    "\n",
    "[Return to Exercise II.6](#Exercise-II.6)\n",
    "\n",
    "There is a third data set: a marketing data set that contains information for each customer on their loyalty program membership, a buyer rating provided by the sales force, and their customer satisfaction rating based on an annual customer satisfaction survey.  The marketing data are in a *csv* file named *marketing.csv*.  You have to:\n",
    "\n",
    "> 1. import the marketing data into a DataFrame (name it df_marketing) and\n",
    "> 2. merge the order_cust DataFrame and this marketing DataFrame.\n",
    "\n",
    "The *CID* is the same in both data sets so it is the linking variable.  Name the final merged DataFrame *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the marketing data\n",
    "## Name this imported data df_marketing\n",
    "##\n",
    "df_marketing = pd.read_csv( path + 'marketing.csv' )\n",
    "df_marketing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to merge the df_orders_cust and df_marketing DataFrames\n",
    "## Name the new merged DataFrame df\n",
    "##\n",
    "df = pd.merge( df_orders_cust, df_marketing, on = 'CID' )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check the shape of df\n",
    "##\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check number of unique CIDs in df\n",
    "##\n",
    "data = len( df_orders.CID.unique() )\n",
    "print( 'Number of unique CIDs: {}'.format( data ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.7\n",
    "\n",
    "[Return to Exercise II.7](#Exercise-II.7)\n",
    "\n",
    "Using your merged orders/customers DataFrame, *df*, create a summary statistics display.  What is the skewness of the Total Discount (Tdisc)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Discount is slightly left skewed since the distance between Q! and the median is bigger than the distance between Q3 qnd the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.1\n",
    "\n",
    "[Return to Exercise III.1](#Exercise-III.1)\n",
    "\n",
    "Examine the distribution of pocket price using a histogram.  What can you conclude?  Redo using a log transformation.  Now what do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot( df.Pprice )\n",
    "ax.set( title = \"Pocket Price Distribution\", \n",
    "       xlabel = 'Pocket Price',\n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot( np.log1p( df.Pprice) )\n",
    "ax.set( title = \"Pocket Price Distribution\\nLog Scale\", \n",
    "       xlabel = 'Pocket Price (Natural Log)',\n",
    "       ylabel = 'Proportions' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.2\n",
    "\n",
    "[Return to Exercise III.2](#Exercise-III.2)\n",
    "\n",
    "Check the distribution of the pocket price by marketing region, loyalty program membership, and buyer rating. What do you conclude?  A complete Data Dictionary is [here](#Appendix-Complete-Data-Dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for pocket price by region.\n",
    "##\n",
    "ax = sns.boxplot( x = \"Region\", y = \"Pprice\", data = df )\n",
    "ax.set( title = 'Pocket Price Distribution\\nMarketing Region', \n",
    "       xlabel = 'Marketing Region',\n",
    "      ylabel = 'Pocket Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for pocket price by loyalty program.\n",
    "##\n",
    "ax = sns.boxplot( x = \"loyaltyProgram\", y = \"Pprice\", data = df )\n",
    "ax.set( title = 'Pocket Price Distribution\\nLoyalty Program Membership', \n",
    "       xlabel = 'Loyalty Program Membership',\n",
    "      ylabel = 'Pocket Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by buyer rating.\n",
    "##\n",
    "ax = sns.boxplot( x = \"buyerRating\", y = \"Pprice\", data = df, order = [ 'Poor', 'Good', 'Excellent' ] )\n",
    "ax.set( title = 'Pocket Price Distribution\\nLBuyer Rating', \n",
    "       xlabel = 'Buyer Rating',\n",
    "      ylabel = 'Pocket Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.3\n",
    "\n",
    "Create a random sample of $n = 1000$ and plot unit sales vs. total discounts (*Tdics*).  What do you conclude?\n",
    "\n",
    "[Return to Exercise III.3](#Exercise-III.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Draw a random sample of size n = 1000\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = df.sample( n = 1000, random_state = 1234 )\n",
    "##\n",
    "## Plot the data using the random sample\n",
    "##\n",
    "ax = sns.regplot( x = 'Tdisc', y = 'Usales', data = smpl )\n",
    "ax.set( title = 'Unit Sales vs. Total Discounts\\nRandom Sample\\nn = 1000', \n",
    "       ylabel = 'Unit Sales', xlabel = 'Total Discounts' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.4\n",
    "\n",
    "[Return to Exercise III.4](#Exercise-III.4)\n",
    "\n",
    "Study the relationship between Total Discount (*Tdisc*) and the pocket price (*Pprice*).  Use a random sample of $n = 200$, a Lowess smooth, and omit the scatter points.  Let the pocket price be on the vertical axis.  What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a sample of n = 200\n",
    "##\n",
    "smpl = df.sample( n = 200, random_state = 1234 )\n",
    "##\n",
    "## Plot\n",
    "##\n",
    "ax = sns.regplot( x = 'Tdisc', y = 'Pprice', data = smpl, lowess = True, scatter = False )\n",
    "ax.set( title = 'Pocket Price vs Total Discount\\nRandom Sample\\nn = 200\\nWith Lowess Smooth', \n",
    "       xlabel = 'Total Discount', ylabel = 'Pocket Price' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.1\n",
    "\n",
    "Merge the X and y testing data sets for predicting.\n",
    "\n",
    "[Return to Exercise IV.1](#Exercise-IV.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data sets for predicting.\n",
    "## Use an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable Usales.\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n{}'.format( test.head() ) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.2\n",
    "\n",
    "Add log Usales and log Pprice to the testing data set.\n",
    "\n",
    "[Return to Exercise IV.2](#Exercise-IV.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for the testing data\n",
    "##\n",
    "test[ 'log_Usales' ] = np.log1p( test.Usales )\n",
    "test[ 'log_Pprice' ] = np.log1p( test.Pprice )\n",
    "print( 'Testing Data Set:\\n{}'.format( test.head() ) )\n",
    "print( '\\nTestng Data Set Shape:\\n{}'.format( test.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.3\n",
    "\n",
    "[Return to Exercise IV.3](#Exercise-IV.3)\n",
    "\n",
    "Estimate a new OLS model by adding the buyer rating to the above model. Name your model regE01.  Interpret your results.  Is the buyer rating important for sales?\n",
    "\n",
    "Hint: Buyer rating is categorical so you have to create dummies for the rating.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "## The formula uses a “~” to separate the left-hand side from the right-hand side\n",
    "## of a model and a “+” to add columns to the right-hand side.  A “-” sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_Usales ~ log_Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region ) + C( buyerRating )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model <===\n",
    "##\n",
    "mod = smf.ols( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##      Recommendation: number your models\n",
    "##      This numbering includes an \"E\" for \"Exercise\"\n",
    "##\n",
    "regE01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( regE01.summary() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The buyer rating is highly insignificant so this variable is not important and can be omitted in a next iteration of estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.4\n",
    "\n",
    "[Return to Exercise IV.4](#Exercise-IV.4)\n",
    "\n",
    "Test the Null Hypothesis that all the buyer rating estimated parameters are zero.  That is, there is no difference among the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## F-test for the buyer ratings\n",
    "##\n",
    "hypothesis = ' ( C(buyerRating)[T.Good] = 0, C(buyerRating)[T.Poor] = 0 ) '\n",
    "##\n",
    "## Run an F-test and retrieve the p-value\n",
    "##\n",
    "f_test = regE01.f_test( hypothesis )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "##\n",
    "## Print results\n",
    "##\n",
    "print( 'p-value for F-Test: {}'.format( pval ) )\n",
    "if pval < 0.05:\n",
    "    print( '\\nSignificant so reject H0' )\n",
    "else:\n",
    "    print( '\\nInsignificant so do not reject H0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Buyer Rating is highly insignificant (i.e., do not reject the Null Hypothesis) because the p-value is 0.68 which is greater than 0.05.  We already know this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.5\n",
    "\n",
    "Create a scenario with the following settings for your model with buyer Rating (regE01):\n",
    "\n",
    "- 'Pprice': [ 2.50 ]\n",
    "- 'Ddisc': [ 0.03 ]\n",
    "- 'Odisc': [ 0.03 ]\n",
    "- 'Cdisc': [ 0.03 ]\n",
    "- 'Pdisc': [ 0.03 ]\n",
    "- 'buyerRating': [ 'Poor' ]\n",
    "- 'Region': [ 'South' ]\n",
    "\n",
    "[Return to Exercise IV.5](#Exercise-IV.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dict\n",
    "##\n",
    "data = {\n",
    "         'Pprice': [ 2.50 ],\n",
    "         'Ddisc': [ 0.03 ],\n",
    "         'Odisc': [ 0.03 ],\n",
    "         'Cdisc': [ 0.03 ],\n",
    "         'Pdisc': [ 0.03 ],\n",
    "         'buyerRating': [ 'Poor' ],\n",
    "         'Region': [ 'South' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_Pprice',\n",
    "                value = np.log1p( scenario.Pprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = regE01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}