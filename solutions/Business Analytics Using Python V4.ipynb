{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red>Introduction to Business Analytics:<br>Using Python for Better Business Decisions</font>\n",
    "\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* Walter R. Paczkowski, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## slide code\n",
    "##\n",
    "from IPython.display import Image\n",
    "def slide(what):\n",
    "    display( Image( \"../Slides/BA_Page_\" + what + \".png\", width = 50, height = 50, retina = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [**_Helpful Background_**](#Helpful-Background)\n",
    "    1. [About this Notebook](#About-this-Notebook)\n",
    "    2. [Helpful Online Tutorials](#Helpful-Online-Tutorials)\n",
    "    3. [Helpful/Must-Read Book](#Helpful/Must-Read-Book)\n",
    "2. [**_Lesson I Introduction to Business Anaytics_**](#Lesson-I-Introduction-to-Business-Anaytics)\n",
    "3. [**_Lesson II Simple Analytics: Understanding and Preparing Your Data_**](#Lesson-II-Simple-Analytics:-Understanding-and-Preparing-Your-Data)\n",
    "    1. [II.1 Documenting Your Data and Workflow: Best Practices](#II.1-Documenting-Your-Data-and-Workflow:-Best-Practices)\n",
    "        1. [II.1.1 Documenting Your Data](#II.1.1-Documenting-Your-Data)\n",
    "        2. [Exercise II.1](#Exercise-II.1)\n",
    "        2. [II.1.2 Documenting Your Workflow](#II.1.2-Documenting-Your-Workflow)\n",
    "            1. [II.1.2.1 Jupyter Notebooks: Overview](#II.1.2.1-Jupyter-Notebooks:-Overview)\n",
    "    2. [II.2 Importing Python Packages](#II.2-Importing-Python-Packages)\n",
    "        1. [II.2.1 Introduction to Python Packages](#II.2.1-Introduction-to-Python-Packages)\n",
    "        2. [II.2.2 Load Packages](#II.2.2-Load-Packages)\n",
    "        3. [II.2.3 Accessing a Function in a Package](#II.2.3-Accessing-a-Function-in-a-Package)\n",
    "    3. [II.3 Importing Your Data Into Pandas](#II.3-Importing-Your-Data-Into-Pandas)\n",
    "        1. [II.3.1 Set Data Path](#II.3.1-Set-Data-Path)\n",
    "        2. [II.3.2 Import Data](#II.3.2-Import-Data)\n",
    "    4. [II.4 Cleaning and Wrangling Your Data](#II.4-Cleaning-and-Wrangling-Your-Data)\n",
    "    5. [II.5 Manipulating Your Data](#II.5-Manipulating-Your-Data)\n",
    "        1. [II.5.1 Creating Variables](#II.5.1-Creating-Variables) \n",
    "        2. [Exercise II.2](#Exercise-II.2)\n",
    "        3. [Exercise II.3](#Exercise-II.3)\n",
    "        4. [Exercise II.4](#Exercise-II.4)\n",
    "        5. [Exercise II.5](#Exercise-II.5)\n",
    "        6. [II.5.2 Merge or Join DataFrames](#II.5.2-Merge-or-Join-DataFrames)\n",
    "        7. [Exercise II.6](#Exercise-II.6)\n",
    "    6. [II.6 Summary Statistics for Your Data](#II.6-Summary-Statistics-for-Your-Data)            \n",
    "        1. [Exercise II.7](#Exercise-II.7)\n",
    "    7. [II.7 What is Next?](#II.7-What-is-Next?)\n",
    "6. [**_Lesson III Data Visualization for Insight_**](#Lesson-III-Data-Visualization-for-Insight)\n",
    "    1. [III.1 Look at the Distribution of Your Data](#III.1-Look-at-the-Distribution-of-Your-Data)\n",
    "        1. [III.1.1 Histograms](#III.1.1-Histograms)\n",
    "            1. [Exercise III.1](#Exercise-III.1)\n",
    "        2. [III.1.2 Boxplots](#III.1.2-Boxplots)\n",
    "            1. [Exercise III.2](#Exercise-III.2) \n",
    "    5. [III.2 Look for Relationships in Your Data](#III.2-Look-for-Relationships-in-Your-Data)\n",
    "        1. [III.2.1 Transformation for Better Interpretation](#III.2.1-Transformation-for-Better-Interpretation)\n",
    "        2. [III.2.2 Enhancing the Scatter Plot](#III.2.2-Enhancing-the-Scatter-Plot)\n",
    "        3. [III.2.3 Adding a Categorical Variable](#III.2.3-Adding-a-Categorical-Variable)\n",
    "        4. [III.2.4 Working with *Large-N* Data](#III.2.4-Working-with-Large-N-Data)    \n",
    "            1. [III.2.4.1 Random Sampling](#III.2.4.1-Random-Sampling)\n",
    "            2. [Exercises III.3](#Exercise-III.3)\n",
    "            3. [III.2.4.2 Contour Plot](#III.2.4.2-Contour-Plot)\n",
    "            4. [III.2.4.3 Hex Bin Plot](#III.2.4.3-Hex-Bin-Plot)\n",
    "            5. [III.2.4.4 Lowess Curve](#III.2.4.4-Lowess-Curve)\n",
    "            6. [Exercises III.4](#Exercise-III.4)\n",
    "    7. [III.3 Look for Trends in Your Data](#III.3-Look-for-Trends-in-Your-Data)         \n",
    "    8. [III.4 Look for Patterns in Your Data](#III.4-Look-for-Patterns-in-Your-Data)\n",
    "    9. [III.5 Look for Anomalies in Your Data](#III.5-Look-for-Anomalies-in-Your-Data) \n",
    "    10. [III.6 What is Next?](#III.6-What-is-Next?)        \n",
    "6. [**_Lesson IV Predictive Modeling: Introduction to Machine Learning_**](#Lesson-IV-Predictive-Modeling:-Introduction-to-Machine-Learning)\n",
    "    1. [IV.1 Comparing and Contrasting Prediction and Forecasting](#IV.1-Comparing-and-Contrasting-Prediction-and-Forecasting)\n",
    "    2. [IV.2 Steps for Predictive Modeling](#IV.2-Steps-for-Predictive-Modeling)\n",
    "        1. [IV.2.1 Steps for Predictive Modeling: Train/Test Split Data](#IV.2.1-Steps-for-Predictive-Modeling:-Train/Test-Split-Data)\n",
    "            1. [Exercise IV.1](#Exercise-IV.1)\n",
    "            2. [Exercise IV.2](#Exercise-IV.2)\n",
    "        2. [IV.2.2 Steps for Predictive Modeling: Train a Model](#IV.2.2-Steps-for-Predictive-Modeling:-Train-a-Model)\n",
    "           1. [Case I Continuous Dependent Variable: OLS Regression](#Case-I-Continuous-Dependent-Variable:-OLS-Regression)\n",
    "              1. [Exercise IV.3](#Exercise-IV.3)           \n",
    "              2. [Case I Analyze the Results](#Case-I-Analyze-the-Results)\n",
    "              3. [Exercise IV.4](#Exercise-IV.4)\n",
    "              4. [Case I Predict with the Model](#Case-I-Predict-with-the-Model)\n",
    "              5. [Exercise IV.5](#Exercise-IV.5)\n",
    "           2. [Case II Binary Dependent Variable: Logistic Regression](#Case-II-Binary-Dependent-Variable:-Logistic-Regression)\n",
    "              1. [Case II Create Your Data](#Case-II-Create-Your-Data)\n",
    "              2. [Case II Train a Model](#Case-II-Train-a-Model)\n",
    "              3. [Case II Predict with the Model](#Case-II-Predict-with-the-Model)\n",
    "           2. [Case III Constants: Decision Trees](#Case-III-Constants:-Decision-Trees)\n",
    "              1. [Case III Check Model Accuracy](#Case-III-Check-Model-Accuracy)\n",
    "              2. [Case III Display the Tree](#Case-III-Display-the-Tree)\n",
    "              3. [Exercise IV.6](#Exercise-IV.6)\n",
    "7. [**_Lesson V Summary and Wrap-up_**](#Lesson-V-Summary-and-Wrap\\-up)\n",
    "7. [**_Contact Information_**](#Contact-Information)\n",
    "8. [**_Appendix_**](#Appendix)\n",
    "    1. [Appendix I.1 Different Ways to Import Data Into Pandas](#Appendix-I.1-Different-Ways-to-Import-Data-Into-Pandas)\n",
    "    2. [Appendix I.2 Some Additional Information on Checking Your Data](#Appendix-I.2-Some-Additional-Information-on-Checking-Your-Data)\n",
    "        1. [Task \\#1 Display the First Few Records of Your DataFrame](#Task-\\#1-Display-the-First-Few-Records-of-Your-DataFrame)\n",
    "        2. [Task \\#2 Check the Shape of Your DataFrame](#Task-\\#2-Check-the-Shape-of-Your-DataFrame)\n",
    "        3. [Task \\#3 Check the Column Names in Your DataFrame](#Task-\\#3-Check-the-Column-Names-in-Your-DataFrame)\n",
    "        4. [Task \\#4 Check for Missing Data in Your DataFrame](#Task-\\#4-Check-for-Missing-Data-in-Your-DataFrame)\n",
    "    3. [Appendix I.3 Miscellaneous Pandas DataFrame Column Manipulations](#Appendix-I.3-Miscellaneous-Pandas-DataFrame-Column-Manipulations)\n",
    "        1. [Deleting Columns](#Deleting-Columns)\n",
    "    4. [Appendix I.4 Correlation Analysis](#Appendix-I.4-Correlation-Analysis)\n",
    "    5. [Appendix II.1 Data Visualization](#Appendix-II.1-Data-Visualization)\n",
    "        1. [Additional Histogram Methods](#Additional-Histogram-Methods)\n",
    "        2. [Additional Boxplot Methods](#Additional-Boxplot-Methods)\n",
    "        3. [Additional Scatter Plot Methods](#Additional-Scatter-Plot-Methods)\n",
    "           1. [Categorical Variable](#Categorical-Variable)\n",
    "           2. [Panel Plots](#Panel-Plot)\n",
    "           3. [Combining Scatter Plots and Histograms](#Combining-Scatter-Plots-and-Histograms)\n",
    "           4. [Pairwise Scatter Plots](#Pairwise-Scatter-Plots)\n",
    "           5. [Contour Plots with Density Functions](#Contour-Plots-with-Density-Functions)\n",
    "        4. [Additional Time Series Plot Methods](#Additional-Time-Series-Plot-Methods)\n",
    "    6. [Appendix III.1 Extra Material for Predictive Modeling](#Appendix-III.1-Extra-Material-for-Predictive-Modeling)\n",
    "        1. [Check OLS Model for Multicollinearity](#Check-OLS-Model-for-Multicollinearity)\n",
    "        2. [Case I Model Portfolio](#Case-I-Model-Portfolio)\n",
    "        3. [Case II Model Portfolio](#Case-II-Model-Portfolio)\n",
    "    7. [Appendix Complete Data Dictionary](#Appendix-Complete-Data-Dictionary)\n",
    "9. [**_Exercise Solutions_**](#Exercise-Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Background\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Notebook\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This notebook accompanies the PDF presentation ***Business Analytics: Using Python for Better Business Decisions*** by Walter R. Paczkowski, Ph.D. (2019).  There is more content and commentary in this notebook than in the presentation deck.  Nonetheless, the two complement each other and so should be studied together.  Every effort has been made to use the same key slide titles in the presentation deck and this notebook which will help your studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Online Tutorials\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"http://docs.python.org/2/tutorial/\" target=\"_parent\">Python Tutorial</a>\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\" target=\"_parent\">Pandas Tutorial</a>\n",
    "\n",
    "* <a href=\"https://seaborn.pydata.org/tutorial.html\" target=\"_parent\">Seaborn Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.statsmodels.org/stable/index.html\" target=\"_parent\">Statsmodels Tutorial</a>\n",
    "\n",
    "\n",
    "### Helpful/Must-Read Book\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&tag=quantpytho-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491957662&linkId=8c3bf87b221dbcd8f541f0db20d4da83\" target=\"_parent\">Main Pandas go-to book: </a> *Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython* (2nd Edition) by Wes McKinney.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## slide code\n",
    "##\n",
    "from IPython.display import Image\n",
    "def slide(what):\n",
    "    display( Image( \"../Slides/BA_Page_\" + what + \".png\", width = 50, height = 50, retina = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson I Introduction to Business Anaytics\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '003' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '005' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '007' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '009' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '010' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '011' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson II Simple Analytics: Understanding and Preparing Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '014' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '016' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '017' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Documenting Your Data and Workflow: Best Practices\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "There are two things to do to document your work:  \n",
    "\n",
    "1. document your data; and\n",
    "2. document your workflow.\n",
    "\n",
    "A _Data Dictionary_ is used for the first and a _Jupyter notebook_ is used for the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.1 Documenting Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The first task in any data analysis is data documentation, perferably in the form of a *Data Dictionary*.\n",
    "\n",
    "A data dictionary contains *metadata* which are data about the data.  Metadata can be anything that helps you understand the data you're using.  Based on [Wikipedia](http://en.wikipedia.org/wiki/Metadata), metadata are information about the distinct data items, such as:\n",
    "\n",
    "1. means of creation;\n",
    "2. purpose of the data;\n",
    "3. time and date of creation;\n",
    "4. creator/author/keeper of the data;\n",
    "5. placement on a network (electronic form);\n",
    "6. where the data were created;\n",
    "7. what standards were used to create the data; and \n",
    "8. etc.\n",
    "\n",
    "I'll restrict the metadata to:\n",
    "\n",
    "1. Variable name;\n",
    "2. Possible values or value ranges;\n",
    "3. Source; and \n",
    "4. Mnemonic.\n",
    "\n",
    "The mnemonic is the label used in data files and statistical and modeling output.  \n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "We will soon import customer specific data that has four columns: \n",
    "\n",
    "1. CID: the customer ID;\n",
    "2. State: the 50 US states plus Washington, DC;\n",
    "3. ZIP: the 5-digit US ZIP (postal) code; and\n",
    "4. Region: the marketing region which corresponds to the four US Census Regions (Midwest, Northeast, South, and West).\n",
    "\n",
    "Create a Data Dictionary assuming this data come from the marketing department.  Use the labels *CID, State, ZIP*, and *Region* as the mnemonics.  Enter the Data Dictionary in a Markdown cell.\n",
    "\n",
    "[See Solution II.1](#Solution-II.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the Data Dictionary here:\n",
    "\n",
    "| Variable                     | Values                              | Source       | Mnemonic          |\n",
    "|------------------------------|-------------------------------------|--------------|-------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.2 Documenting Your Workflow\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Documenting your workflow is as important as documenting your data.  This documentation will enable you to reproduce your work and make it easier for a colleague to follow what you did.  The *Jupyter notebook* paradigm is the best platform for this documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II.1.2.1 Jupyter Notebooks: Overview\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Jupyter is a sophisticated programming tool sometimes called an *ecosystem*.  It is an ecosystem because it will handle a number of programming languages, Python being one of them.  The langauges are called _kernels_.  Other kernels are R, Julia, Fortran to mention a few.  There are over 100 kernels supported by Jupyter.  Jupyter was originally created to handle Julia, Python, and R.  In fact, the name \"Jupyter\" is a contraction for **JU**lia, **PYT**hon, and **R**.\n",
    "\n",
    "The paradigm for Jupyter is a lab notebook used in the physical sciences for laboratory experiment documentation.  The Jupyter notebook consists of cells where text and programming code are entered and executed or *run*.  There are two basic cells:\n",
    "\n",
    "1. Code cell; and \n",
    "2. Markdown cell.\n",
    "\n",
    "A code cell is where programming code is entered and executed.  The results are displayed in another cell that appears immediately below a code cell.  To organize the code and result, the pair of cells are labeled with a marker for the code and the result.  The code marker is In[] and the result marker is Out[].  Inside the square brackets are numbers representing the sequence in which code is executed.  The numbers will vary, and even be out of order, if a code cell is executed several times but a following code cell is not reexecuted.  The sequencing numbers can be reset by rerunning the kernel.  This is done by clicking on *Kernel* on the main toolbar and selecting *Restart & Run All*.  \n",
    "\n",
    "The markdown cell is where documentation is entered.  This cell, in fact, is a markdown cell.  It is that you use as many markdown cells as possible to document your work.  You will see many examples of this below.<br>\n",
    "\n",
    "A code cell is the default.  You make a code cell a markdown cell by using the drop-down menu list on the main toolbar.  Of course, you can change a markdown cell to a code cell using the same drop-down list.  You could also use keyboard short-cuts.  See *Help/Keyboard Shortcuts* on the main menu bar. \n",
    "\n",
    "You can easily insert/delete cells and move them from one location in your notebook to another.  To insert a cell above the current cell, select the cell as the base for the insert and click on the colored bar on the left until it turns blue.  Then type *A* to insert a cell above the current cell.  Type *B* to insert one below it.  Type *DD* (two *D*'s) to delete the current cell.  To move a cell, select the cell you want to move, click the colored bar on the left, and use the up-arrow and down-arrow icons on the main menu toolbar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "This code block loads the necessary Python packages for this course.  I recommend setting options, such as those for graphs and print, as was done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Importing Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Python is a powerful programming language that allows you to perform all standard programming operations in a clear and consistent manner.  Its strength, adhered to by Python programers, is a coding format that emphasizes readable code.  Indentation is the primary way to accomplish this.  Also, its strength is based on a very wide array of *packages* or *modules* or *libraries*.  Packages perform analysis or data manipulation operations.  There are many packages, each one providing a special set of analysis tools so a package can be viewed as a container of functions.  Sometimes a package contains smaller, more specialized packages so a grand package could be a container for smaller ones.  You will see how to access and use packages and subset packages in this and other lessons.\n",
    "\n",
    "Pandas is a data manipulation and graphing package with a lot of capabilities.  It will be used extensively in these lessons.  Seaborn is a scientific graphing package that is intuitive to use.  Although Pandas has visualization methods, Seaborn is preferred because of its quality, extent, and easier syntax.  Both packages use Matplotlib for base graphing functions.  Statsmodels has an array of statistical modeling functions, only a few of which will be used in these lessons.  Numpy and Matplotlib are base packages for Pandas, Seaborn, and Statsmodels.  Except for a few functions, Numpy and Matplotlib will not be used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1 Introduction to Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '021' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '023' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '024' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2 Load Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You have to load a package before you can use it.  Loading is done using an *import* command.  The alias is assigned when you import the package.  I recommend loading all the basic packages at once at the beginning of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Data Management <===\n",
    "##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "##\n",
    "## ===> Visualization <===\n",
    "##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "## Set the seaborn grid style.  The dot between the seaborn alias,\n",
    "## \"sns\", and the set() function connects or \"chains\" the alias and the function.\n",
    "##\n",
    "sns.set()\n",
    "##\n",
    "## Set an option for the number of Pandas columns to display.  Eight in this case.\n",
    "## \n",
    "pd.set_option( 'display.max_columns', 8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3 Accessing a Function in a Package\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "A function in a package can be accessed by telling Python the package where the function is located and, of course, the name of the function.  These two operations are done with one statement by *chaining* the package name and the function name.  The chain is formed by connecting the package name and the function name by inserting a dot between the two.  Usually, the package alias is used for improved readability.  An example of a chained command is:\n",
    "<br><br>\n",
    "***pd.read_csv( 'lesson1.csv' )***\n",
    "<br><br>\n",
    "where \"pd\" is the alias for Pandas and \"read_csv\" is a Pandas function that reads a *CSV* file (\"lesson1.csv\" in this example). Notice the dot(\".\") between the alias and the function name.  The dot is the chaining operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Importing Your Data Into Pandas\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Before you can begin any work, you must first import and examine the structure of your data.  This structure is a rectangular array or matrix or, in Pandas terminology, a *DataFrame*.  When you import your data using Pandas, the imported data immediately goes into a DataFrame.  This is very convenient because Seaborn and StatsModels functions recognize these DataFrames.\n",
    "\n",
    "Pandas provides a set of very flexible import functions.  Which one you should use depends on your data format.  Some typical formats and relevant functions are:\n",
    "\n",
    "| Data Format | Pandas Import Function |\n",
    "|----------|---------------|\n",
    "| CSV       | read_csv       |\n",
    "| Excel     | read_excel     |\n",
    "| Clipboard | read_clipboard |\n",
    "| SQL       | read_sql       |\n",
    "| JSON      | read_json      |\n",
    "| SAS       | read_sas       |\n",
    "\n",
    "Some of these will demonstrated below.\n",
    "\n",
    "I will first import a *CSV* formatted file.  The package alias must be \"chained\" with the *read_csv* import function, otherwise Python will not know where to find the read function. \n",
    "\n",
    "When you import data, you must always specify the file path so Pandas can find the file.  If the data file is in the same directory as the notebook, then a path is unnecessary since Pandas always begins a search in the same directory as the notebook.  Otherwise, you have to specify the path.\n",
    "\n",
    "An example of data import is shown below.  Several more are shown in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data path as shown here; keep the format *r'path'* if necessary.  Remember, if your data are in the same directory as your notebook, then a path is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.1 Set Data Path\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is best practice to define paths in one location.  This makes error finding and changes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set data path\n",
    "##\n",
    "path = r'../Data/Furniture/Final Data Files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.2 Import Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and import or read the data.\n",
    "##\n",
    "file = 'orders.csv'\n",
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "df_orders = pd.read_csv( path + file, parse_dates = [ 'Tdate' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The CSV file is specified using the string literal: *r'../Data/furniture/final data files/orders.csv'*.  The *\"r\"* at the begiining of the string tells the Python interpreter to treat this string as raw text that is not to be changed.  Without the *\"r\"*, the interpreter would treat any backslashes as escape characters which would change the meaning of the string.  Even though forward slashes are used in this code block, it is good practice to avoid issues and use the *\"r\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## How many unique CIDs are available\n",
    "##\n",
    "x = len( df_orders.CID.unique() )\n",
    "print( 'Number of unique CIDs: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4 Cleaning and Wrangling Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '026' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 1: Display first few records.\n",
    "##\n",
    "df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The \"head\" method is attached to a DataFrame when you create it.  The default is to display the first five records.  You could use *n = 10* as an argument to display the first 10 records: *df.head( n = 10 )*.  You could display the last five records using the *tail* method.  The default is also five which could be changed as for the *head* method.  For both *head* and *tail*, the number of columns displayed is set using *pd.set_option( 'display.max_columns', 8 )* as was done in the package loading section above.  The *head* and *tail* methods are chained to the DataFrame name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 2: Check the shape of the data\n",
    "##\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_orders.shape[0], cols = df_orders.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*shape* is an attribute of the DataFrame so it does not require parentheses; it does not have any arguments.  Functions and methods have arguments (which may be defaults) so parentheses are required.  The *shape* attribute is chained to the DataFrame name.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The *shape* attribute returns the number of rows and the number of columns in that order.  The *orders* DataFrame has 70,270 *rows* or *records* or *observations* and 14 *columns* or *variables* or *features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 3: Check the column names\n",
    "##\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*columns* is an attribute of the DataFrame.\n",
    "\n",
    "**_Interpretaion_**\n",
    "\n",
    "When checking the column names, be sure there are no white spaces before and after the name.  White spaces can (and will) cause problems because your tendency will be to write a column name without the leading and trailing white spaces; Python will then not recognize the name.  If you see leading and trailing white spaces, you can remove them using the following:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.strip()*\n",
    "\n",
    "where *str* is the string package which is part of the base Python kernel and automatically loaded with Python.  You may also want to convert the column names to all lower case:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.lower()*\n",
    "\n",
    "You could do both at once using:\n",
    "\n",
    "*df_orders.columns = df_orders.columns.str.strip().str.lower()*\n",
    "\n",
    "Notice the use of *str* twice in this last expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Task 4: Check for Missing\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "*info* is a method chained to the DataFrame.  It returns the number of non-missing records for each column plus data types: object (i.e., text string), floating point numbers, integers, and datetime.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "There are 14 columns with 10 having 70,270 nonmissing values while the last four have less than 70,270 so they have mising values.  For example, *Ddisc* (the dealer discount based on the Data Dictionary) has 70,262 records so 8 are missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5 Manipulating Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '028' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.5.1 Creating Variables\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '030' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate total discount.\n",
    "##\n",
    "## Discounts are sometimes called \"leakages\" so the total is \n",
    "## the total leakage.\n",
    "##\n",
    "## Note: use \"axis = 1\" in the sum() function to sum across columns.\n",
    "## This allows you to do the summation even with missing values.\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )\n",
    "##\n",
    "## Display only the discounts\n",
    "##    Create a list of what to print. \n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Tdisc' ]\n",
    "df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanantion_**\n",
    "\n",
    "The *sum* method has an axis argument that specifies the axis the function is to be applied on.  *axis = 0* specifies summing along the rows for each column (i.e., sum down a column) and *axis = 1* specifies summing along the columns in each row.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Notice the *NaN* values.  *NaN* stands for *Not a Number*.  These are missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The *Pocket Price* is the list price less total discounts or total leakages.  It is the amount the business \"pockets\" and is the amount the customer actually pays.  The pocket price formula is $Pprice = Lprice \\times (1  - Tdisc)$.  Calculate the pocket price for the orders DataFrame and display the first five records for the list price and pocket price.\n",
    "\n",
    "[See Solution](#Solution-II.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Calculate total revenue as $Rev = Usales \\times Pprice$.  Use the df_orders DataFrame.\n",
    "\n",
    "[See Solution](#Solution-II.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "*Contribution* and *contribution margin* are two values financial analysts often examine.  Contribution is comparable to what economists call *profit* but is more restricted in that it just refers to a product without considering any fixed or overhead costs.  Contribution is $Con = Revenue - Material~Cost$ and contribution margin is $CM = \\dfrac{Con}{Revenue}$.  Calculate both quantities and display the first 5 records of unit sales, pocket price, material cost, revenue, contribution, and contribution margin.  Use the df_orders DataFrame.\n",
    "\n",
    "[See Solution](#Solution-II.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.5\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Some products are returned so another revenue number, *revenue net of returns*, is more meaningful and revealing for business decisions.  Net revenue is\n",
    "<br><br>\n",
    "$Net Revenue = (Unit Sales - Returns) \\times Pocket Price$.\n",
    "<br><br>\n",
    "Calculate net revenue and call it 'netRev'.  Also calculate the loss in revenue due to the returns and call it 'lostRev'.  Display the first five records of the DataFrame using just gross revenue, net revenue, and the lost revenue due to returns.  Use the *df_orders* DataFrame.  \n",
    "\n",
    "[See Solution](#Solution-II.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.5.2 Merge or Join DataFrames \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is not unusual to have data in two (or more) tables so you will need to merge or join them to get all the data you need for an analysis.  For our problem, a second data table has information on each customer and this second table must be merged with the orders table.  The merge is done on the customer *ID* (*CID*).  There are many types of joins but we will only use an *inner join* in the examples.  *Inner join* is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '032' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = 'customers.csv'\n",
    "df_cust = pd.read_csv( path + file )\n",
    "df_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df_orders_cust = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "##\n",
    "df_orders_cust.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The merge function takes two arguments: the *left* table and the *right* table to merge or join.  The tables are in that order.  A third argument specifies what to merge on.  There are several options for the *on* variable.  In this example, the *on* variable is just the common key in each table: *CID*.\n",
    "\n",
    "An alternative form for the merge statement is:\n",
    "\n",
    "*df = df_orders.merge( df_cust, on = 'CID' )*\n",
    "\n",
    "An *inner join* is the default.  \n",
    "\n",
    "See the Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" target=\"_parent\">here</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\" target=\"_parent\">here</a> for extensive discussion with examples about this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Check the shape of the new DataFrame against \n",
    "## that of the orders and customers DataFrames\n",
    "##\n",
    "print( \"Shape of the orders DataFrame: {}\\n\".format( df_orders.shape ) )\n",
    "print( \"Shape of the customers DataFrame: {}\\n\".format( df_cust.shape ) )\n",
    "print( \"Shape of the new DataFrame: {}\\n\".format( df_orders_cust.shape ) )\n",
    "##\n",
    "x = len( df_orders.CID.unique() )\n",
    "print( 'Number of unique CIDs: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "A variation on the print function is used: the *format* command.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "*df_orders* has 21 columns, *df_cust* has 4 columns, while the merged *df_orders_cust* has 24.  The difference of one column is the *CID* which is in both and is the linking variable; it's only included once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise II.6\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "There is a third data set: a marketing data set that contains information for each customer on their loyalty program membership, a buyer rating provided by the sales force, and their customer satisfaction rating based on an annual customer satisfaction survey.  The marketing data are in a *csv* file named *marketing.csv*.  You have to:\n",
    "\n",
    "1. import the marketing data into a DataFrame (name it df_marketing) and\n",
    "2. merge the order_cust DataFrame and this marketing DataFrame.\n",
    "\n",
    "The *CID* is the same in both data sets so it is the linking variable.  Name the final merged DataFrame *df*.\n",
    "\n",
    "[See Solution](#Solution-II.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to import the marketing data\n",
    "## Name this imported data df_marketing\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to merge the order_cust and df_marketing DataFrames\n",
    "## Name the new merged DataFrame df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check the shape of df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here to check number of unique CIDs in df\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.6 Summary Statistics for Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Summary statistics are a mainstay for starting any analysis.  Pandas has all the usual descriptve statistics.  One function, *describe()* will display the essential ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '034' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: \"describe\" is a method attached to the DataFrame so it requires ().\n",
    "## Round to 1 decimal place for readability (more decimal places are\n",
    "## unnecessay, anyway).\n",
    "##\n",
    "## Display the descriptive statistics for the discounts.\n",
    "##    First create a list of variables to display.\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "df[ x ].describe().round( 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *round* function is chained to the *describe* method.  An alternative way to round is shown next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above report for the descriptive statistics is a challenge to read.  I prefer to have the statistics as the columns.  This can easily be done once you recognize that the report is just a matrix.  Matrices can be transposed which could help you read the report more easily.  Use the \"*T*\" attribute to transpose a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example of transposed matrix and alternative\n",
    "## round function use.\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc', 'Tdisc' ]\n",
    "round( df[ x ].describe().T, 1 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "From the *Five Number Summary* (min/25%/50%/75%/max), you can determine the skewness of your data.\n",
    "\n",
    "1. Symmetric: $(75\\% - 50\\%) = (50\\% - 25\\%)$\n",
    "2. Right Skewed: $(75\\% - 50\\%) > (50\\% - 25\\%)$\n",
    "3. Left Skewed: $(75\\% - 50\\%) < (50\\% - 25\\%)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** What is the skewness for the Dealer Discount (*Ddisc*)?\n",
    "\n",
    "**ANSWER**: right skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.7\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Using your merged orders/customers/marketing DataFrame, *df*, create a summary statistics display.  What is the skewness of the Total Discount (Tdisc)?\n",
    "\n",
    "[See Solution](#Solution-II.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.7 What is Next?\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "In Lesson III, I will show you how to do some basic graphing or *visualization* of your data.  This may seem more like scientific visualization than business visualization.  The latter is usually *infographics* which is not useful for gaining insight and, hence, useful Rich Information.  The former, scientific visualization, is the tool for extracting Rich Information. \n",
    "<br><br><br>\n",
    "<font color = red, size = \"+3\"><b> Five Minute Break </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson III Data Visualization for Insight\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '037' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Specifically, you will learn to use:\n",
    "\n",
    "1. histograms;\n",
    "2. boxplots;\n",
    "3. scatter plots;\n",
    "4. contour plots; and\n",
    "5. hex bin plots\n",
    "\n",
    "to visualize your data.  The focus is on scientific visualization rather than infographics visualization.      \n",
    "\n",
    "**Case Study Problem**:\n",
    "<br><br>\n",
    "The product manager wanted to know about unit sales and discounts by:\n",
    "\n",
    "1. Overall Market\n",
    "2. Marketing Region\n",
    "3. Customer Loyalty\n",
    "4. Buyer Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '039' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '040' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '042' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '044' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1 Look at the Distribution of Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '046' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.1 Histograms\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can use a histogram to examine the distribution of unit sales and the total discount.  Notice in the following display that a smooth line is overlayed.  This is a *kernel density estimate* (*KDE*).  You will see this again shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '047' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '048' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Histogram of unit sales\n",
    "##\n",
    "ax = sns.distplot( df.Usales )\n",
    "ax.set( title = \"Unit Sales Distribution\", xlabel = 'Unit Sales', \n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Plotting a histogram is very easy.  The Seaborn *distplot* command is used with the argument set to the variable of interest.  The plot is saved in a variable called \"ax\".  Parameters such as title and labels can be passed to this variable.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The distribution is highly skewed to the right which distorts the impression of the data.  Using the natural log will normalize the display.  This is helpful so when you model unit sales you should use a log transformation.  This next graph shows that the distribution (on a log scale) is fairly normal.\n",
    "\n",
    "**_Recommendation_**\n",
    "    \n",
    "Use the Numpy *log1p* function.  This returns the natural log of one plus the argument: $np.log1p( x ) = log_e(1 + x)$.  The reason for using this function is to avoid cases where $x = 0$: $log(0)$ is undefined, which is meaningless, but $log( 1 ) = 0$ so you would have a meaningful number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the natural log of unit sales\n",
    "## A KDE curve is included by default\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales) )\n",
    "ax.set( title = \"Unit Sales Distribution: Log Scale\", \n",
    "       xlabel = 'Unit Sales (Natural Log)',\n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The natural log transformation changed the distribution to a more normal looking distribution.  Normality is preferred for statistical analysis for a host of reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Examine the distribution of pocket price using a histogram.  What can you conclude?  Redo using a log transformation.  Now what do you conclude?\n",
    "\n",
    "[See Solution](#Solution-III.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for pocket price.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for transformed pocket price.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.2 Boxplots\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Boxplots are the most useful visualization tool for examining distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '049' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '050' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the boxplot for total discounts\n",
    "##\n",
    "ax = sns.boxplot( y = 'Tdisc', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount', ylabel = 'Total Discount' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that the Seaborn boxplot function only has an argument for the y-axis.  In this case, the x-axis is understood.  This gives a vertical chart as shown.  However, if you change the \"y\" to \"x\", the boxplot will be horizontal: *sns.boxplot( x = 'Tdisc', data = df )* produces a horizontal chart.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The Total Discount is symmetrically distributed.  This is evident by an almost mirror image above and below the center line inside the box.  The center line is the median.  This boxplot is for the entire market.  But what about regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region', ylabel = 'Total Discount', \n",
    "       xlabel = 'Marketing Regions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "In this drill-down of the total discounts by marketing regions, the Seaborn boxplot function now has two axis arguments: \n",
    "\n",
    "1. y-axis; and\n",
    "2. x-axis (*Region* in this case).\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Notice that discounts are the lowest in the Southern Region while the Midwest has a large number of very low discounts.  Also, the dispersion of the discounts in the Southern Region is small relative to that in the other three regions.  Let us drill down on the discounts to verify the differences for the Southern Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drill down on the discounts in the Southern Region\n",
    "##\n",
    "## Select the discounts for the Southern Region\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "df_south = df.loc[ df.Region == 'South', x ]\n",
    "##\n",
    "## Use a boxplot to examine the distributions.\n",
    "##\n",
    "ax = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt( df_south ) )\n",
    "ax.set( title = 'Discount Distribution\\nSouthern Marketing Region', \n",
    "       xlabel = 'Type of Discount',\n",
    "      ylabel = 'Discount Amount')\n",
    "##\n",
    "## Reset the tick labels to more meaningful labels\n",
    "##\n",
    "ax.set_xticklabels( [ 'Dealer', 'Order\\nSize', 'Competitive', 'Pickup' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the dealer discount tends to be the largest while the order discount has the most variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Examine the distribution of net revenue by region, loyalty program, and buyer rating using boxplots.  What can you conclude?  A complete Data Disctionary is [here](#Appendix-Complete-Data-Dictionary).\n",
    "\n",
    "[See Solution](#Solution-III.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by region.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by loyalty program.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for net revenue by buyer rating.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2 Look for Relationships in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Scatter plots are the workhorse of statistical displays because they allow you to see relationships -- sometimes.  Properly drawn, they can provide a wealth of insight into: \n",
    "\n",
    "- relationships;\n",
    "- trends;\n",
    "- patterns; and\n",
    "- anomalies\n",
    "\n",
    "of two continuous variables.  They can be supplemented with histograms on the margins to show distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '052' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '053' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '054' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1 Transformation for Better Interpretation\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Since one objective from the product manager is to estimate a price elasticity, you should graph unit sales and Pocket Price.  We noticed earlier that unit sales were right skewed but that using a log transform shifted the distribution to a more normal one.  We should take the log of unit sales as well as pocket price.  This is a very common transformation in empirical demand analysis because the slope of a line is the elasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Transform unit sales and pocket price\n",
    "##\n",
    "df[ 'log_Pprice' ] = np.log1p( df.Pprice )\n",
    "df[ 'log_Usales' ] = np.log1p( df.Usales )\n",
    "##\n",
    "## Display the unlogged and logged data\n",
    "##\n",
    "x = [ 'Pprice', 'log_Pprice', 'Usales', 'log_Usales' ]\n",
    "df[ x ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the logged data\n",
    "## Use the Seaborn \"relplot\" function\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', xlabel = 'Log Pocket Price', \n",
    "       ylabel = 'Log Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "A negative relationship is evident -- as it should be.  But the large number of plot points makes it slightly difficult to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.2 Enhancing the Scatter Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Replot the logged data with a regression line added. \n",
    "## Use the Seaborn \"regplot\" function.\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "## Note: \n",
    "##   The plot element colors can be set:\n",
    "##     b:blue, g:green, r:red, c:cyan,\n",
    "##     m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                 scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"yellow\"} )\n",
    "##                 color = 'y' )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Log Pocket Price', ylabel = 'Log Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The Seaborn *regplot* function is used to add a regression line to the scatter plot.  To help distinguish between plotting points and the regression line, the *scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"yellow\"}* arguments are used.  The points are specified as black and the line as yellow.  The default is for both to be the same color.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The regression line shows a negative relationship between price and sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3 Adding a Categorical Variable\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can add a third variable that is categorical to show relationships across groups.  This is done with a \"hue\" command which colors the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Loyalty Program membership\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'loyaltyProgram', \n",
    "                 data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Log Pocket Price', \n",
    "       ylabel = 'Log Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4 Working with *Large-N* Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The scatter plots are dense, making it difficult to see patterns. Options are to use a:\n",
    "\n",
    "1. random sample;\n",
    "2. contour plot;\n",
    "3. hex bin plot; or\n",
    "4. Lowess smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4.1 Random Sampling\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '056' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Draw a random sample of size n = 500\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = df.sample( n = 500, random_state = 1234 )\n",
    "##\n",
    "## Plot the data using the random sample\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = smpl )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nRandom Sample\\nn = 500', \n",
    "       ylabel = 'Log Unit Sales', xlabel = 'Log Pocket Price' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The Pandas DataFrame method *sample* is used to draw a random sample of size $n = 500$.  The random seed is set at 1234 so the same sample would be drawn each time the cell is run.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The negative relationship between unit sales and price is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a random sample of $n = 1000$ and plot unit sales vs. total discounts (Tdics).  What do you include?\n",
    "\n",
    "[See Solution](#Solution-III.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4.2 Contour Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '057' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '058' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contour plot with margnal distributions\n",
    "## Sample\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = smpl, kind = \"kde\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Seaborn's *jointplot* is used.  The *kind = kde* argument is used for a *kernel density plot* which is the contours.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The dark spot in the middle shows the concentration of the data points.  The negative relationship between sales and price is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4.3 Hex Bin Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '059' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Hex binning\n",
    "## Sample\n",
    "##\n",
    "## Note: A white background is best for this \n",
    "## Note: \n",
    "##   The plot element colors can be set: \n",
    "##     b:blue, g:green, r:red, c:cyan,\n",
    "##     m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "with sns.axes_style( 'white' ):\n",
    "    ax = sns.jointplot(x = 'log_Pprice', y = 'log_Usales', data = smpl, \n",
    "                       kind=\"hex\", color = 'k' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4.4 Lowess Curve\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "An alternative is to fit a *Lowess Smooth* to the data.  *Lowess* stands for *Locally Weighted Scatterplot Smooth*.  It is a regression fit and approaches the *OLS* line for very smooth fits.  See <a href=\"https://en.wikipedia.org/wiki/Local regression\" target=\"_parent\">here</a> for a description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fit a Lowess Smooth with the scatter turned off\n",
    "## Sample\n",
    "##\n",
    "ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = smpl, lowess = True, scatter = False )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nRandom Sample\\nn = 500\\nWith Lowess Smooth', \n",
    "       ylabel = 'Log Unit Sales', xlabel = 'Log Pocket Price' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Study the relationship between Total Discount (*Tdisc*) and the pocket price (*Pprice*).  Use a random sample of $n = 200$, a Lowess smooth, and omit the scatter points.  Let the pocket price be on the vertical axis.  What can you conclude?\n",
    "\n",
    "[See Solution](#Solution-III.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3 Look for Trends in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Trends are identified using line graphs, usually with time on the X-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '061' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the date indicator and the Dealer discount\n",
    "##\n",
    "x = [ 'Tdate', 'Ddisc' ]\n",
    "tmp = df_orders[ x ]\n",
    "##\n",
    "## Reset the index to the date\n",
    "##\n",
    "tmp.set_index( 'Tdate', inplace = True )\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The subset DataFrame containing *Tdate* and *Ddisc*, *tmp*, is reindexed using *Tdate*.  *Tdate* was converted to a DateTime variable when the orders data were originally imported. \n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The data for *Ddisc* are by year-month-day.  Notice that there are missing values indicated by *NaN*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Group the data by months and calculate the \n",
    "## mean discount for each month.\n",
    "##\n",
    "grp = tmp.resample( 'M' ).mean()\n",
    "grp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *tmp* DataFrame is *resampled* to monthly data, the resampling using the mean of values in each month.  Basically, *resample* aggrgegates the data by the datetime index.  See <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" target=\"_parent\">here</a> for documentation on *resample*.\n",
    "\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Each value is the mean of values for the indicated month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Uses Pandas' plot function.\n",
    "## It automatically uses the time index for the X-axis.\n",
    "##\n",
    "ax = grp.plot( y = 'Ddisc' , legend = False )\n",
    "ax.set( title = 'Dealer Discount\\nMonthly', ylabel = 'Dealer Discount', xlabel = 'Months' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Pandas does not connect points if a point is missing. Pandas gives a better representation and is better with time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4 Look for Patterns in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Patterns are identified using a variety of visual displays.  So all the graph types discussed will help identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '063' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5 Look for Anomalies in Your Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '065' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Categorical plot: boxplot\n",
    "##\n",
    "ax = sns.catplot( 'Tdisc', kind = 'box', orient = 'v', data = df_orders )\n",
    "ax.set( title = 'Total Discount\\nOutliers', ylabel = 'Total Discount', xlabel = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There are some clear outliers:\n",
    "\n",
    "1. A number of points are very low.\n",
    "2. Only one or two points are very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.6 What is Next?\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "In Lesson IV, I will show you how to build two predictive models:\n",
    "\n",
    "1. *OLS*;\n",
    "2. Logit; and\n",
    "3. Decision trees.\n",
    "\n",
    "I'll discuss these in the next lesson.\n",
    "<br><br><br>\n",
    "<font color = red, size = \"+3\"><b> Five Minute Break </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson IV Predictive Modeling: Introduction to Machine Learning\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '068' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1 Comparing and Contrasting Prediction and Forecasting\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '070' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2 Steps for Predictive Modeling\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '072' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2.1 Steps for Predictive Modeling: Train/Test Split Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '074' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '075' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '076' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are split into two parts using *sklearn*.  Each part has a *X* variable array and a *y* vector (The upper and lower cases are conventional).  The *X* array is a Pandas DataFrame of the *X* variables.  The *y* vector is a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import train_test_split package\n",
    "##\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Notice the cases for the variable names.\n",
    "##\n",
    "y = df[ 'Usales' ]\n",
    "##\n",
    "x = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region', 'buyerRating' ]\n",
    "X = df[ x ]\n",
    "##\n",
    "## Split the data.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The dependent and independent variables need to be separated from the main DataFrame before the train/test split can be done.  The index from the main DataFrame is preserved.  The first three lines of code do this.  The *train_test_split* function randomly divides the data, keeping the indexes aligned.  The *random_state = 42* argument sets the random seed.  Four data sets are returned which are (in order): *X_train, X_test, y_train*, and *y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print(\"Sample sizes: \\nX: {}, y: {}\\n\".format( X_train.shape[0], y_train.shape[0] ) )\n",
    "print( 'Training X Data: \\n{}'.format( X_train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training y Data: \\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Note the indexes for the training and testing data sets.  These are the same as the main DataFrame, *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: Usales\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( train.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *X* and *Y* training data sets are merged on the indexes.  Recall that the index were preserved when the *y* and *X* data sets were created.  This is why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the X and y testing data sets for predicting.\n",
    "\n",
    "[See Solution](#Solution-IV.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add log Usales and log Pprice to the training data\n",
    "## The log is based on the Numpy function log1p\n",
    "## Note: log1p( x ) = log( 1 + x )\n",
    "##\n",
    "train[ 'log_Usales' ] = np.log1p( train.Usales )\n",
    "train[ 'log_Pprice' ] = np.log1p( train.Pprice )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training Data Set Shape:\\n {}'.format( train.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Logged terms are added because the Data Visualization showed that logs induce normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Add log Usales and log Pprice to the testing data set.\n",
    "\n",
    "[See Solution](#Solution-IV.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here.\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2.2 Steps for Predictive Modeling: Train a Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "I will cover three predictive models:\n",
    "\n",
    "1. *OLS*\n",
    "2. Logit\n",
    "3. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '078' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '079' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '080' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '081' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '083' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '084' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case I Continuous Dependent Variable: *OLS* Regression\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Model unit sales as a function of the pocket price to get a price elasticity.  Recall that you are using log terms and that the estimated coefficient for log price is the elasticity.\n",
    "\n",
    "**Recommendation**:  Use formulas to specify the model.  You need the *statsmodels.formula* api for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '086' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '087' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## For modeling, notice the new import command for\n",
    "## the formula API and the summary option\n",
    "##\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "## The formula uses a ~ to separate the left-hand side from the right-hand side\n",
    "## of a model and a + to add columns to the right-hand side.  A - sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_Usales ~ log_Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your fitted models\n",
    "##\n",
    "reg01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( reg01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The modeling follows four steps as shown above.  Regardless of the software you might use, these same four steps are followed.  Some software combines them, others require explicit statement.  This is what statsmodels requires.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The price elasticity is the coefficient for the logged price variable (i.e., log_Pprice): -1.7.  If price falls by 1\\%, unit sales rise by 1.7\\%.  This indicates that blinds are highly elastic.  This should be expected since furniture is a competitive business and blinds are very competitive.  Revenue will also change.  If price fall, revenue will increase.  The amount revenue will increase (in percentage terms) is given by $1 + elasticity$.  So for a 1\\% fall in price, revenue will rise 0.7\\% (= $1 + [-1.7]$). \n",
    "\n",
    "The discounts and regions seem to have no effect, but this can be tested as we'll do below.  Also note that the $R^2$ is 0.20 which is very low.  \n",
    "\n",
    "The Jarque-Bera Test is a test for normality of the disturbance term.  It is a test of the \"goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. $\\ldots$ The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.  $\\ldots$ If it is far from zero, it signals the data do not have a normal distribution.\"  So the Null Hypothesis is $H_O: Normality$.  (Source: <a href=\"https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test\" target=\"_parent\">see here</a>)  In this case, the Null is rejected.  The Omnibus Test is an alternative test of normality with the same Null.  It also indicates that the Null must be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Estimate a new OLS model by adding the buyer rating to the above model. Name your model regE01.  Interpret your results.  Is the buyer rating important for sales?\n",
    "\n",
    "**Hint**: Buyer rating is categorical so you have to create dummies for the rating.\n",
    "\n",
    "[See Solution](#Solution-IV.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case I Analyze the Results\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Quantities of interest can be extracted directly from the fitted model. Type *dir(results)* for a full list.\n",
    "\n",
    "Since the product manager wanted to know about a region effect, you should do an F-test of all the coefficients for the regions to determine if they are all zero, meaning that the dummies as a group do nothing.  This is a <u>joint</u> test of significance.  The test statistic is:\n",
    "\n",
    "$F_C = \\dfrac{\\left(SSR_U - SSR_R\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)} = \\dfrac{\\left(SSE_R - SSE_U\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)}$\n",
    "\n",
    "where \"U\" indicates the *unrestricted* or *full* model with the Region dummies and \"R\" indicates the *restricted* model without the Region dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify the joint (Null) hypothesis that the regions are the same;\n",
    "## i.e., there is no region effect.\n",
    "##\n",
    "hypothesis = ' ( C(Region)[T.Northeast] = 0, C(Region)[T.South] = 0, C(Region)[T.West] = 0 ) '\n",
    "##\n",
    "## Run an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "##\n",
    "## Print results\n",
    "##\n",
    "print( 'p-value for F-Test: {}'.format( pval ) )\n",
    "if pval < 0.05:\n",
    "    print( 'Significant so reject H0' )\n",
    "else:\n",
    "    print( 'Insignificant so do not reject H0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that there are only three regions specified even though there are four: one is omitted as the base.  Also notice that the three hypotheses are specified as *C(Region)[T.XX] = 0* where *XX* is the region name.\n",
    "\n",
    "**_Output Interpretation_**\n",
    "\n",
    "There are several returned values for the F-test.  Only the p-value is important.\n",
    "\n",
    "**_Interpetation_**\n",
    "\n",
    "The Null Hypothesis is that there is no region effect.  The p-value is 0.32 so the Null Hypothesis is not rejected: there is no Region effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat the F-test for the discounts\n",
    "##\n",
    "hypothesis = ' ( Ddisc = 0, Odisc = 0, Cdisc = 0, Pdisc = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "##\n",
    "## Print results\n",
    "##\n",
    "print( 'p-value for F-Test: {}'.format( pval ) )\n",
    "if pval < 0.05:\n",
    "    print( 'Significant so reject H0' )\n",
    "else:\n",
    "    print( 'Insignificant so do not reject H0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The hypothesis statement does not have the discount names as *C(Ddis)* etc. because they are quantitative variables, not categorical variables like *Region*. \n",
    "\n",
    "The Null Hypothesis is that there is no difference among the discounts; they all have zero effect on unit sales.  Notice that the p-value is 0.34.  So the Null Hypothesis that the discounts all have the same effect is not rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.4\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Test the Null Hypothesis that all the buyer rating estimated parameters are zero.  That is, there is no difference among the ratings.\n",
    "\n",
    "[See Solution](#Solution-IV.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case I Predict with the Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Predict unit sales.  Recognize that sales are in (natural) log terms.  You will convert back to unit sales in \"normal\" terms later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate predicted log of unit sales, the dependent variable.\n",
    "##\n",
    "## Note: the inverse of the log is needed; use np.expm1( x )\n",
    "## since log1p was used: np.expm1 = exp(x) - 1.\n",
    "##\n",
    "log_pred = reg01.predict( test )\n",
    "y_pred = np.expm1( log_pred )\n",
    "##\n",
    "##\n",
    "## Combine into one temporary DataFrame for convenience\n",
    "##\n",
    "tmp = pd.DataFrame( { 'y_test':y_test, 'y_logPred':log_pred, 'y_pred':y_pred } )\n",
    "tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the sklearn metrics function *r2_score* to check the fit of actual vs. predicted values.  From the sklearn User Guide:\n",
    "\n",
    "\"*The r2_score function computes R, the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import the r2_score function from the sklearn metrics package\n",
    "##\n",
    "from sklearn.metrics import r2_score\n",
    "##\n",
    "## Display the r2 score.  But first drop any NaN data.\n",
    "##\n",
    "tmp.dropna( inplace = True )\n",
    "print( 'r2 Score:\\n {}'. format( round( r2_score( tmp.y_test, tmp.y_pred), 3 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also graph the actual vs predicted values.  Sometimes, however, the number of data points is too large to plot so a random sample may be needed.  This is our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Draw a random sample of 500 observations without replacement\n",
    "## from the tmp DataFrame.\n",
    "##\n",
    "smpl = tmp.sample( n = 500, replace = False, random_state = 1 )\n",
    "##\n",
    "## Plot the data\n",
    "##\n",
    "ax = sns.regplot( x = 'y_test', y = 'y_pred', scatter = True, data = smpl )\n",
    "ax.set( title = 'Actual vs Predicted Units Sales\\nRandom Sample of 500', \n",
    "       ylabel = 'Predicted Sales', xlabel = 'Actual Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unit sales for different settings of the variables.  This is *scenario* or *what-if* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dict\n",
    "##\n",
    "data = {\n",
    "         'Pprice': [ 2.50 ],\n",
    "         'Ddisc': [ 0.03 ],\n",
    "         'Odisc': [ 0.05 ],\n",
    "         'Cdisc': [ 0.03 ],\n",
    "         'Pdisc': [ 0.03 ],\n",
    "         'Region': [ 'West' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_Pprice',\n",
    "                value = np.log1p( scenario.Pprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = reg01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV.5\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a scenario with the following settings for your model with buyer Rating (regE01):\n",
    "\n",
    "- 'Pprice': [ 2.50 ]\n",
    "- 'Ddisc': [ 0.03 ]\n",
    "- 'Odisc': [ 0.03 ]\n",
    "- 'Cdisc': [ 0.03 ]\n",
    "- 'Pdisc': [ 0.03 ]\n",
    "- 'buyerRating': [ 'Poor' ]\n",
    "- 'Region': [ 'South' ]\n",
    "\n",
    "[See Solution](#Solution-IV.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case II Binary Dependent Variable: Logistic Regression\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '089' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '090' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '091' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Create Your Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is part of the DataFrame.  Satisfaction is measured on a five-point scale: *1 = Not at All Satisfied*, *5 = Very Satisfied*.  \n",
    "\n",
    "First, look at the frquency count of satisfaction.  But, there is a problem: you cannot use the same data as before since satisfaction is by customer and the data used so far are by transaction.  The satisfaction rating is in the customer DataFrame.  You need to first find the mean price and mean discounts by customer from the transactions DataFrame and then merge this new DataFrame with the customer DataFrame.  So, there are several steps:\n",
    "\n",
    "1. Extract the pocket price and discounts -- include the *CID*\n",
    "2. Group by the CID and calculate the means by *CID*\n",
    "3. Merge with the customer DataFrame\n",
    "4. Recode the scale values in the merged file so that 1 is the top-two values (called *top-two box* or *T2B*) and 0 is all other values.  The *T2B* is *Very Satisfied*.\n",
    "5. Train a model with *T2B* satisfaction as a function of the pocket price, discounts, and Region.\n",
    "\n",
    "The Customer Satisfaction variable is documented in the complete Data Disctionary [here](#Appendix-Complete-Data-Dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## ===> Step 1: Extract the pocket price and discounts -- include the CID\n",
    "##\n",
    "tmp = df[ [ 'CID', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ] ]\n",
    "tmp.set_index( 'CID', inplace = True )\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = tmp.shape[0], cols = tmp.shape[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 2: Group by the CID and calculate the means by CID\n",
    "##\n",
    "x = tmp.groupby( 'CID' ).mean()\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = x.shape[0], cols = x.shape[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Look at the head of x\n",
    "##\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 3: Merge with the customer and marketing DataFrames\n",
    "##              Merge on the CID\n",
    "##\n",
    "df_sat = pd.merge( pd.merge( x, df_cust, on = 'CID' ), df_marketing, on = 'CID' )\n",
    "##\n",
    "## Alternative merge:\n",
    "## df_sat = x.merge( df_cust, on = 'CID' ).merge( df_marketing, on = 'CID' )\n",
    "## \n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_sat.shape[0], cols = df_sat.shape[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the columns\n",
    "##\n",
    "df_sat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do a quick check of the value distribution.\n",
    "##\n",
    "## Use the DataFrame's value_counts() method. Sort by the\n",
    "## scale values 1 - 5.\n",
    "##\n",
    "x = df_sat.buyerSatisfaction.value_counts( normalize = True, sort = False )\n",
    "print( 'Buyer Satisfaction Ratings: \\n{}'.format( x ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 4: Recode the scale values so that 1 is the top-two values \n",
    "## (called \"top-two box\" or \"T2B\") and 0 is all other values.  \n",
    "## The \"T2B\" is \"Very Satisfied\".\n",
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## ===> Step 4.A: Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 1, 0 ]\n",
    "##\n",
    "## ===> Step 4.B: Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( df_sat.buyerSatisfaction >= 4 ),\n",
    "    ( df_sat.buyerSatisfaction < 4 )\n",
    "]\n",
    "##\n",
    "## ===> Step 4.C: Do the recoding \n",
    "##\n",
    "df_sat[ 'sat_t2b' ] = np.select( conditions, lbl )\n",
    "##\n",
    "df_sat[ 'sat_t2b' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *T2B* satisfaction as a function of the pocket price and discounts.  First, create training and testing DataFrames as before but with *sat_t2b* as the *y* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 5: Train a model.\n",
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df_sat[ 'sat_t2b' ]\n",
    "x = [ 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc', 'Region' ]\n",
    "X = df_sat[ x ]\n",
    "##\n",
    "## Split the data.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data: training data\n",
    "##\n",
    "print(\"Sample sizes: \\nX: {}, y: {}\\n\".format( X_train.shape[0], y_train.shape[0] ) )\n",
    "print( 'Training X Data: \\n{}'.format( X_train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training y Data: \\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data: testing data\n",
    "##\n",
    "print(\"Sample sizes: \\nX: {}, y: {}\\n\".format( X_test.shape[0], y_test.shape[0] ) )\n",
    "print( 'Testing X Data: \\n{}'.format( X_test.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Testing y Data: \\n{}'.format( y_test.head() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_train } )\n",
    "train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of the merged training data\n",
    "##\n",
    "print(\"Sample size:\\n {}\".format( train.shape[0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of the merged testing data\n",
    "##\n",
    "print(\"Sample size:\\n {}\".format( test.shape[0] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Train a Model\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Train a logit model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "formula = 'sat_t2b ~ Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region )'\n",
    "##\n",
    "## ===> Step 2: Instantiate the logit model\n",
    "##\n",
    "mod = smf.logit( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case II Predict with the Model\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The prediction process is the same as discussed for *Case I* above, but now you can compare actuals and predicted using a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import needed functions\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "##\n",
    "## Make predictions\n",
    "##\n",
    "predictions = logit01.predict( test )\n",
    "predictions_nominal = [ 0 if x < 0.5 else 1 for x in predictions]\n",
    "print( classification_report( y_test, predictions_nominal, digits = 3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix( y_test, predictions_nominal ).ravel()\n",
    "##\n",
    "## zip the variable names and the confusion\n",
    "##\n",
    "lbl = [ 'tn', 'fp', 'fn', 'tp' ]\n",
    "##\n",
    "## display the zip matrix\n",
    "##\n",
    "from statsmodels.compat import lzip\n",
    "lzip( zip( lbl, x ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There were 1 true negatives, 81 false positives, 3 false negatives, and 173 true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Satisfied', 'Satisfied']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal )\n",
    "tmp = pd.DataFrame(data=cm, index = lbl, columns = lbl )\n",
    "print( 'Confusion Matrix: \\n{}'.format( tmp ) )\n",
    "##\n",
    "## Plot the confusion matrix\n",
    "##\n",
    "sns.set( font_scale = 1.4 )   #for label size\n",
    "##\n",
    "ax = sns.heatmap( cm/cm.sum(), annot = True, annot_kws = { \"size\": 16 } )  # font size\n",
    "ax.set( title = 'Confusion Matrix for the Classifier', xlabel = 'Predicted',\n",
    "       ylabel = 'True' )\n",
    "ax.set_xticklabels(lbl)\n",
    "ax.set_yticklabels(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "67\\% of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case III Constants: Decision Trees\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Decision Trees can handle continuous or discrete dependent variables.  They are an alternative to *OLS* and logistic regression: you don't have to specify a \"model\" *per se*.  They also have the advantage that a visual display, a *tree*, is produced which is easier for management and clients to understand than complex regression output and statistics.  You will only look at a discrete case; a continuous case is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '093' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '094' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '095' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '096' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '097' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '098' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import decision tree classifier\n",
    "##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "##\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "##\n",
    "## Convert \"Region\" to integers: the decision tree must have all numerics\n",
    "## Note 1: use the LabelEncoder function for this\n",
    "## Note 2: \"Region\" will be encoded in alphanumeric order:\n",
    "##\n",
    "##          0: Midwest\n",
    "##          1: Northeast\n",
    "##          2: South\n",
    "##          3: West\n",
    "##\n",
    "print( '\\nTraining Data before recoding Region:\\n \\n{}'.format( X_train.head() ) )\n",
    "##\n",
    "## Encode the Region labels in the training and testing data sets using Label Encoder\n",
    "##\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_train[ 'Region' ] = labelencoder.fit_transform( X_train[ 'Region'] )\n",
    "print( '\\nTraining Data after recoding Region:\\n \\n{}'.format( X_train.head() ) )\n",
    "##\n",
    "X_test = X_test.apply( le.fit_transform )\n",
    "print( 'Testing Data: \\n{}'.format( X_test.head() ) )\n",
    "##\n",
    "## Instantiate the tree\n",
    "##\n",
    "dtree = tree.DecisionTreeClassifier( random_state = 0, max_depth = 3, \n",
    "                                    min_samples_leaf = 5 )\n",
    "##\n",
    "## Fit the tree\n",
    "##\n",
    "dtree.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case III Check Model Accuracy\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## The score attribute\n",
    "##\n",
    "print( \"Accuracy on training data: {:.3f}\".format( dtree.score( X_train, y_train )))\n",
    "print( \"Accuracy on testing data: {:.3f}\".format( dtree.score( X_test, y_test )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are good scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Case III Display the Tree\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional packages are needed to plot a decision tree:\n",
    "\n",
    "- graphviz\n",
    "- pydotplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Both packages may have to be installed before they can be used.  \n",
    "## Use the operating system to do this.\n",
    "##\n",
    "import os\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "##\n",
    "## Tell Python where the graphviz package is load; then load it.\n",
    "##\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "##\n",
    "## Load the following packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import needed packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "##\n",
    "## Displaying a tree is a slight challenge!\n",
    "## There are four steps:\n",
    "##\n",
    "## ===> Step 1: Create a placeholder for all the plotting points.\n",
    "##\n",
    "dot_data = StringIO()\n",
    "##\n",
    "## ===> Step 2: Extract the feature names for labels models\n",
    "##\n",
    "feature_names = [ i for i in X_train.columns ]\n",
    "##\n",
    "## ===> Step 3: Export the plotting data to the placeholder\n",
    "##\n",
    "export_graphviz(dtree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                class_names = [ 'Not Satisfied', 'Satisfied' ],\n",
    "                feature_names = feature_names ,\n",
    "                proportion  = True\n",
    "               )\n",
    "##\n",
    "## ===> Step 4: Create the display\n",
    "##\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "For the right node on the third level, \"$Region \\le 2.5$\" is interpreted as *Region* having a value less than or equal to 2.5.  Since $Region = 0$, $Region = 1$, and $Region = 2$ meet this criteria and $Region = 3$ is the West, then if *Region* is Midwest/Northeast/South, go to the left; otherwise, go to the right for the West. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExerciseI V.6\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Interpret the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson V Summary and Wrap-up\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '100' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Information\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '102' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix contains material extra to this lesson, material that you may want to review to solidify your understanding and knowledge about working with Python and Pandas.\n",
    "\n",
    "This Appendix covers:\n",
    "\n",
    "1. Importing data into Pandas\n",
    "2. Checking your data\n",
    "3. Manipulating columns of a Pandas DataFrame;\n",
    "4. Correlation Analysis\n",
    "5. Data Visualization; and \n",
    "6. OLS Modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.1 Different Ways to Import Data Into Pandas\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You could also define the path without the file name if you plan to use multiple files from the same directory.  You just have to define the general path and then concatenate the file name.  Here is an example.  Note:\n",
    "\n",
    "1. the \"+\" which instructs Pandas to add or concatenate the two strings;\n",
    "2. the quotes around the file name; and\n",
    "3. the forward slash as the last symbol in the path definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify a path to the CSV data and concatenate the file name.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../Data/furniture/final data files/'\n",
    "## df_orders = pd.read_csv( path + 'orders.csv' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following form if your data are in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do not specify a path if the CSV file is in the same directory\n",
    "## as your notebook.\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## df_orders = pd.read_csv( 'orders.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are in an Excel file, use *pd.read_excel( filename, 'sheetname' )*.  The sheet name could be a character string or a sheet number as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: Import an Excel file\n",
    "##\n",
    "## Not run\n",
    "##\n",
    "## path = r'../Data/furniture/final data files/'\n",
    "## df_orders = pd.read_excel( path + 'orders.xlsx', 'furniture' )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.2 Some Additional Information on Checking Your Data\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Look at:\n",
    "\n",
    "1. the head of your data;\n",
    "2. the shape of your DataFrame; \n",
    "3. a list of column names; and\n",
    "4. the missingness of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#1 Display the First Few Records of Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the head of the imported data\n",
    "## n = 5 is the default\n",
    "##\n",
    "df_orders.head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should immediately notice that several discounts have missing values indicated by *NaN* (*Not a Number*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#2 Check the Shape of Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: check the shape of the imported data\n",
    "## Note 1: the order of the returned shape is always #rows, #columns\n",
    "## Note 2: there is no () for this command because the shape\n",
    "## is a DataFrame attribute\n",
    "##\n",
    "print( \"Shape of the DataFrame: {}\".format( df_orders.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,270 rows or observations and 15 columns or variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#3 Check the Column Names in Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Example: list the column names\n",
    "##\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Remove white spaces in the column names\n",
    "## White spaces are not an issue here.  This is \n",
    "## just illustrative.\n",
    "##\n",
    "df_orders.columns = df_orders.columns.str.strip()\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \\#4 Check for Missing Data in Your DataFrame\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use the DataFrame's information content.  The info()\n",
    "## method returns the number of non-missing rows for\n",
    "## each variable.  The numbers should be the same.\n",
    "##\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above listing indicates that the four discounts each have missing values. \n",
    "\n",
    "You can count the number of missing values using the *isnull()* method and chaining the *sum()* function.  The *isnull()* method returns a Boolean variable so the *sum()* function just adds 0 and 1 values.  Use a nice print statement for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Sum the Boolean variable returned by isull()\n",
    "## Let us check the Order Discount (Odisc).\n",
    "##\n",
    "x = df_orders.Odisc.isnull().sum()\n",
    "print( 'Missing count for Odisc: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the proportion of each variable that is missing rather than the sum. Proportions are more meaningful. Use the *mean()* function for this.  Since *isnull()* returns a Boolean, the mean is just the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check for missing values only for the discounts.\n",
    "## Chain the mean() function to the isnull() method.\n",
    "## Note: Do this for first 20 records for illustration only.\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ x ].iloc[ :20 ].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a heatmap of missing data using the *isnull()* method on the entire DataFrame.  Use the transpose attribute, *T*, for a more readable chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Note: the \"cbar = False\" argument turns off the color bar\n",
    "## Note: Do this for first 20 records for illustration only\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "ax = sns.heatmap( df_orders[ x ].iloc[ :20, : ].isnull().T, cbar = False )\n",
    "ax.set( title = 'Heatmap of Missing Data' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.3 Miscellaneous Pandas DataFrame Column Manipulations\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting Columns\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can easily delete unwanted columns with the *drop* method.  You must specify if you want the DataFrame replaced or not; the default is to not replace in which case you must assign a new name to the modified DataFrame.  If you drop a column, it is good practice to not replace your DataFrame so that you preserve your original data.\n",
    "\n",
    "The *drop* method can be used to drop rows or columns, so you have to tell it which one.  This is done with the *axis* arugment.  The DataFrame, as a simple rectangular array, is said to have two *axes*: the row axis and the column axis.  Since in mathematics the size of a matrix is conventionally specified as $\\#row \\times \\#columns$ (rows always come before columns), the DataFrame axes are designated as 0 and 1 for rows and columns, respectively, since 0 comes before 1.  Specifying *axis = 0* in the *drop* method says to drop a row while specifying *axis = 1* says to drop a column. The default is *axis=0*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Drop a column and replace the DataFrame.  Notice that axis = 1 is used\n",
    "## to drop a column.\n",
    "## \n",
    "## Not Run\n",
    "##\n",
    "## df_orders.drop( 'obs', axis = 1, inplace = True )\n",
    "## df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix I.4 Correlation Analysis\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "There is a *correlation* method attached to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: display a correlation matrix of the discounts\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "df[ x ].corr().round( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The correlation matrix has a 1.0 in the cells along its main diagonal (the diagonal running from top left to bottom right).  The off-diagonal cells have the pair-wise correlations.  Notice that the correlation matrix is symmetric around the main diagonal: the top portion (called the *upper triangle*) matches the bottom portion (called the *lower triangle*).\n",
    "\n",
    "Round to three decimal places is usually sufficient.\n",
    "\n",
    "The correlations are all very low indicating that the discounts are not linearly associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *heatmap* is sometimes more effective for displaying a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Example: plot the correlation matrix as a heatmap\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "cor = df[ x ].corr()\n",
    "ax = sns.heatmap( cor )\n",
    "ax.set( title = 'Heatmap of the Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The cells along the main diagonal are all white which, by the color bar on the right, indicates they are all 1.0 as they should be.  All other cells are black indicating that the correlations are all 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix II.1 Data Visualization\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix contains material extra to this lesson, material that you may want to review to solidify your understanding and knowledge about working with Python, Pandas, and Seaborn for Data Visualization.\n",
    "\n",
    "This Appendix covers:\n",
    "\n",
    "1. Additional Histogram Methods;\n",
    "2. Additional Boxplot Methods;\n",
    "3. Additional Scatter Plot Methods; and\n",
    "4. Additional Time Series Plot Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Histogram Methods\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a *rug plot* to the bottom of the histogram to show each observation.  This is helpful to show where the data are for each bar in the histogram.  This, of course, is not practical for large data sets since the rug would just be a dense, black bar at the bottom of the graph.  \n",
    "<br>\n",
    "You can also remove the *KDE* curve for a better visualization of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Add a rug and remove the KDE\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales), kde = False, rug = True )\n",
    "ax.set( title = \"Unit Sales Distribution: Log Scale\", \n",
    "       xlabel = 'Unit Sales (Natural Log)', \n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display just the *KDE* curve for a cleaner view of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## KDE only\n",
    "##\n",
    "ax = sns.distplot( np.log1p( df.Usales), hist = False )\n",
    "ax.set( title = \"Unit Sales Distribution: Log Scale\", \n",
    "       xlabel = 'Unit Sales (Natural Log)', \n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Boxplot Methods \n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the discounts by the customer loyalty status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions and Loyalty Program\n",
    "## members\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', hue = 'loyaltyProgram', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region \\n and \\n Loyalty Program',\n",
    "       ylabel = 'Total Discount' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Another view of total discount distribution by Regions and Loyalty Program\n",
    "## members\n",
    "##\n",
    "ax = sns.catplot(x = 'Tdisc', y = 'loyaltyProgram', row = 'Region',\n",
    "                kind = 'box', orient = 'h', height = 1.5, aspect = 4,\n",
    "                data = df )\n",
    "ax.set(  xlabel = 'Total Discount', ylabel = 'Loyalty Program\\nMember'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be disturbing that the discounts are the same whether a customer is in the loyalty program or not.  Members should have bigger discounts.  What about how they are rated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Total discount distribution by regions and buyer rating\n",
    "##\n",
    "ax = sns.boxplot( x = 'Region', y = 'Tdisc', hue = 'buyerRating', data = df )\n",
    "ax.set( title = 'Distribution of Total Discount by Region \\n and \\n Buyer Rating', \n",
    "       ylabel = 'Total Discount' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loyalty and good ratings are not rewarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Scatter Plot Methods \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This Appendix section covers:\n",
    "\n",
    "1. Categorical Variable\n",
    "2. Panel Plot\n",
    "3. Combining Scatter Plots and Histograms\n",
    "4. Pairwise Scatter Plots\n",
    "5. Contour Plots and Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Variable\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Region\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'Region', data = df )\n",
    "ax.set( title = 'Unit Sales vs. Pocket Price\\nLog Scales', \n",
    "       xlabel = 'Pocket Price', ylabel = 'Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Panel Plot\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add Loyalty Program membership\n",
    "## A less cluttered view with panels\n",
    "##\n",
    "## Warning -- this will take a few seconds\n",
    "##\n",
    "ax = sns.relplot( x = 'log_Pprice', y = 'log_Usales', hue = 'loyaltyProgram', \n",
    "                 col = 'Region', col_wrap = 2,\n",
    "                 data = df )\n",
    "ax.set( xlabel = 'Pocket Price', ylabel = 'Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice the gap between 17 and 19 in the Northeast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Scatter Plots and Histograms\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can combine scatter plots with histograms for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add histograms to the margins\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pairwise Scatter Plots\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You can also plot multiple variables in pair-wise combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use the Seaborn pairwise function\n",
    "## Full sample\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "##\n",
    "## We know there are missing values for the discounts.\n",
    "## Missing values are not handled well with Seaborn histograms.\n",
    "## So drop all records with any missing data.\n",
    "##\n",
    "tmp = df[ x ].copy()\n",
    "tmp.dropna( inplace = True )\n",
    "sns.pairplot( tmp[ x ] )\n",
    "##\n",
    "## Warning -- this will take a few minutes\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Unfortunately, this particular plot is clearly not useful because the data set is large; we have a case of *Large-N*.  So how is this handled?  Try a random sample as in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Pairwise plot\n",
    "##\n",
    "## Random sample, n = 500 (previously drawn)\n",
    "##\n",
    "x = [ 'Ddisc', 'Cdisc', 'Odisc', 'Pdisc' ]\n",
    "sns.pairplot( smpl[ x ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "This is not much better.  Maybe a smaller sample will work.  You can try this on your own.  A contour or hex bin plot might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contour Plots with Density Functions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contour plot with margnal distributions\n",
    "## Random sample, n = 500\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "ax = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = smpl, kind = \"kde\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "A different contour plot is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Hex binning\n",
    "##\n",
    "## Random sample, n = 500\n",
    "##\n",
    "## Note: A white background is best for this \n",
    "## Note: The plot element colors can be set:\n",
    "##   b:blue, g:green, r:red, c:cyan,\n",
    "##   m:magenta, y:yellow, k:black, w:white.\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "with sns.axes_style( 'white' ):\n",
    "    ax = sns.jointplot(x = 'log_Pprice', y = 'log_Usales', data = smpl, \n",
    "                       kind=\"hex\", color = 'k' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add a regression line\n",
    "##\n",
    "## Full data sample\n",
    "##\n",
    "## Warning -- this will take a minute\n",
    "##\n",
    "with sns.axes_style(\"white\"):\n",
    "    g = sns.jointplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                      kind = 'hex', color = 'k',\n",
    "                      joint_kws={'gridsize':40, 'bins':'log'} )\n",
    "    ax = sns.regplot( x = 'log_Pprice', y = 'log_Usales', data = df, \n",
    "                     ax = g.ax_joint, scatter = False, color = \"yellow\" )\n",
    "    ax.set( xlabel = 'Log Pocket Price', ylabel = 'Log Unit Sales' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Time Series Plot Methods \n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Time series plot for Southern Region\n",
    "##\n",
    "x = [ 'Tdate', 'Ddisc' ]\n",
    "tmp = df.loc[ df.Region == 'South', x ]\n",
    "##\n",
    "## Reset the index to the date\n",
    "##\n",
    "tmp.Tdate = pd.to_datetime( tmp.Tdate )\n",
    "tmp.set_index( 'Tdate', inplace = True )\n",
    "grp = tmp.resample( 'M' ).mean()\n",
    "##\n",
    "## Create a Month variable from the index\n",
    "##\n",
    "grp['x'] = grp.index\n",
    "grp['Month'] = grp.x.dt.month\n",
    "print(grp.head())\n",
    "##\n",
    "ax = grp.plot( y = 'Ddisc' , legend = False )\n",
    "ax.set( title = 'Dealer Discount\\nMonthly\\nSouthern Region', ylabel = 'Dealer Discount', xlabel = 'Months' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix III.1 Extra Material for Predictive Modeling\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check OLS Model for Multicollinearity\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Multicollinearity is a major issue with high-dimensional datasets.  A high level of multicollinearity can negatively impact estimation results.  It can be checked for using:\n",
    "\n",
    "1. a correlation matrix; or\n",
    "2. a variance inflation factor (VIF) measure.  A rule-of-thumb is that any $VIF > 10$ indicates a problem.\n",
    "\n",
    "This material assumes that the OLS model for Case I was estimated [here](#Case-I-Continuous-Dependent-Variable:-OLS-Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the correlation matrix\n",
    "## \n",
    "## Subset the design matrix to eliminate the first column of 1s\n",
    "## the iloc method says to find the location of columns based on \n",
    "## their integer locations (i.e., 0, 1, 2, etc.)\n",
    "## the term in brackets says to find all rows (the : ) and all \n",
    "## columns from the first to the end (1: )\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ] \n",
    "corr_matrix = x.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Graph the correlation matrix\n",
    "##\n",
    "sns.heatmap( corr_matrix ).set_title( 'Heatmap of the Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## A fancy version of the heatmap\n",
    "## Based on: https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
    "##\n",
    "cmap = sns.diverging_palette( 5, 250, as_cmap = True )\n",
    "##\n",
    "corr_matrix.style.background_gradient( cmap, axis=1 ).set_precision( 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate VIFs\n",
    "##\n",
    "## The VIFs are the diagonal elements of the inverted correlation\n",
    "## matrix of the independent variables.\n",
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s.\n",
    "## The iloc method says to find the location of columns based on their \n",
    "## integer locations (i.e., 0, 1, 2, etc.) the term in brackets says \n",
    "## to find all rows (the : ) and all columns from the first to the end (1: ).\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ]\n",
    "corr_matrix = x.corr()\n",
    "##\n",
    "## Invert the correlation matrix and extract the main diaginal\n",
    "##\n",
    "vif = np.diag( np.linalg.inv( corr_matrix ) ) \n",
    "##\n",
    "## Zip the variable names and the VIFs\n",
    "##\n",
    "indepvars = [ i for i in x.columns ]\n",
    "xzip = zip( indepvars, vif ) \n",
    "##\n",
    "## Display the zip matrix.  First import a needed function:\n",
    "##\n",
    "from statsmodels.compat import lzip\n",
    "lzip( xzip )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *VIF*s are all below 10 so there is no problem.  $VIF > 10$ is a rule-of-thumb for indicating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case I Model Portfolio\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This is a nice way to summarize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Import some packags\n",
    "##\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.api import anova_lm\n",
    "##\n",
    "## Create a variable to hold the model names; this is a list.\n",
    "## Note: the range() function specifies 1 - 2 but the \"2\" is\n",
    "## not included.\n",
    "##\n",
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "              'R2 Adjusted': lambda x: \"{:0.3f}\".format( x.rsquared_adj ),\n",
    "              'AIC': lambda x: \"{:0.2f}\".format( x.aic ),\n",
    "              'F': lambda x: \"{:0.2f}\".format( x.fvalue ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ reg01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case II Model Portfolio\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [ 'Model ' + str( i ) for i in range( 1, 2 ) ]\n",
    "##\n",
    "## Create a variable to hold the statistics to print; this is a dictionary.\n",
    "##\n",
    "info_dict = { '\\nn': lambda x: \"{0:d}\".format( int( x.nobs ) ),\n",
    "}\n",
    "##\n",
    "## Create the portfolio summary table.\n",
    "##\n",
    "summary_table = summary_col( [ logit01 ],\n",
    "            float_format = '%0.2f',\n",
    "            model_names = model_names,\n",
    "            stars = True, \n",
    "            info_dict = info_dict \n",
    ")\n",
    "summary_table.add_title( 'Summary Table for Living Room Blinds Sales' )\n",
    "print( summary_table )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Complete Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                              | Order Sys   | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Customer's State          | 50 US states + DC                      | Marketing Sys| State        |\n",
    "| ZIP Code                  | 5-digit US ZIP (postal) code           | Marketing Sys| ZIP          |\n",
    "| Marketing Region of Customer | Four US Census Regions              | Marketing Sys| Region       |\n",
    "| Member of Loyalty Program | Nominal: Yes/No                        | Marketing Sys| loyaltyProgram |\n",
    "| Rating of Customer        | Nominal: Poor/Good/Excellent           | Marketing Sys| buyerRating |\n",
    "| Cusomer Satisfaction Rating | 5 Point Likert Scale: 5 = Very Sat.  | Marketing Sys| buyerSatisfaction |\n",
    "| Total Discount            | \\%                                     | Calculated: Sum of Discounts | TDisc    |\n",
    "| Pocket Price              | \\$US                                   | Calculated: $Lprice \\times (1  - Tdisc)$| Pprice     |\n",
    "| Revenue                   | \\$US  | Calculated: $USales \\times Pprice$ | Rev |\n",
    "| Net Revenue               | \\$US  | Calculated: $(Usales - returnAmount) \\times Pprice$  | newRev |\n",
    "| Lost Revenue              | \\$US  | Calcualted: $Rev - netRev$    | lostRev |\n",
    "| Profit Contribution       | \\$US  | Calculated: $Rev - Mcost$    | Con |\n",
    "| Contribution Margin       | \\%    | Calculated: $\\dfrac{Con}{Rev}$ | CM |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.1\n",
    "\n",
    "Create a table in Markdown mode.\n",
    "\n",
    "[Return to Exercise II.1](#Exercise-II.1)\n",
    "\n",
    "| Variable                     | Values                              | Source       | Mnemonic          |\n",
    "|------------------------------|-------------------------------------|--------------|-------------------|\n",
    "| Customer ID                  | Nominal                             | Customer Sys | CID               | \n",
    "| Customer's State             | 50 US states + DC                   | Marketing Sys| State             |\n",
    "| ZIP Code                     | 5-digit US ZIP (postal) code        | Marketing Sys| ZIP               |\n",
    "| Marketing Region of Customer | Four US Census Regions              | Marketing Sys| Region            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.2\n",
    "\n",
    "[Return to Exercise II.2](#Exercise-II.2)\n",
    "\n",
    "The *Pocket Price* is the list price less total discounts or total leakages.  It is the amount the business \"pockets\" and is the amount the customer actually pays.  The pocket price formula is $Pprice = Lprice \\times (1  - Tdisc)$.  Calculate the pocket price and display the first five records for the list price and pocket price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )\n",
    "##\n",
    "## Display just the components of Pprice\n",
    "##\n",
    "x = [ 'Lprice', 'Tdisc', 'Pprice' ]\n",
    "df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.3\n",
    "\n",
    "[Return to Exercise II.3](#Exercise-II.3)\n",
    "\n",
    "Calculate total revenue as $Rev = Usales \\times Pprice$.  Use the df_orders DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Multiply Unit Sales and Pocket Price\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice\n",
    "##\n",
    "## Create a list of unit sales, pocket price, and revenue\n",
    "##\n",
    "x = [ 'Usales', 'Pprice', 'Rev' ]\n",
    "df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.4\n",
    "\n",
    "[Return to Exercise II.4](#Exercise-II.4)\n",
    "\n",
    "*Contribution* and *contribution margin* are two values financial analysts often examine.  Contribution is comparable to what economists call *profit* but is more restricted in that it just refers to a product without considering any fixed or overhead costs.  Contribution is $Con = Revenue - Material~Cost$ and contribution margin is $CM = \\dfrac{Con}{Revenue}$.  Calculate both quantities and display the first 5 records of unit sales, pocket price, material cost, revenue, contribution, and contribution margin.  Use the df_orders DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Contribution: Subtract Material Cost (Mcost from the Data Dictionary) from Revenue\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost\n",
    "##\n",
    "## Contribution Margin: Divide Contribution by Revenue\n",
    "##\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev\n",
    "##\n",
    "## Create a list to display\n",
    "##\n",
    "x = [ 'Usales', 'Pprice', 'Mcost', 'Rev', 'Con', 'CM' ]\n",
    "df_orders[ x ].head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.5\n",
    "\n",
    "[Return to Exercise II.5](#Exercise-II.5)\n",
    "\n",
    "Some products are returned so another revenue number, *revenue net of returns*, is more meaningful and revealing for business decisions.  Net revenue is\n",
    "<br><br>\n",
    "$Net~Revenue = (Unit~Sales - Amount~Returned) \\times Pocket~Price$.\n",
    "<br><br>\n",
    "Calculate net revenue and call it 'netRev'.  Also calculate the loss in revenue due to the returns and call it 'lostRev'.  Display the first five records of the DataFrame using just gross revenue, net revenue, and the lost revenue due to returns.  Use the *df_orders* DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Net Revenue: Subtract the amount returned (returnAmount from the Data Dictionary) and \n",
    "## multiply by the Pocket Price\n",
    "##\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice\n",
    "##\n",
    "## Lost Revenue: Total revenue less the net revenue\n",
    "##\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev\n",
    "##\n",
    "## Create a list to display\n",
    "##\n",
    "x = [ 'Rev', 'netRev', 'lostRev' ]\n",
    "df_orders[ x ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.6\n",
    "\n",
    "[Return to Exercise II.6](#Exercise-II.6)\n",
    "\n",
    "There is a third data set: a marketing data set that contains information for each customer on their loyalty program membership, a buyer rating provided by the sales force, and their customer satisfaction rating based on an annual customer satisfaction survey.  The marketing data are in a *csv* file named *marketing.csv*.  You have to:\n",
    "\n",
    "1. import the marketing data into a DataFrame (name it df_marketing) and\n",
    "2. merge the order_cust DataFrame and this marketing DataFrame.\n",
    "\n",
    "The *CID* is the same in both data sets so it is the linking variable.  Name the final merged DataFrame *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the marketing data\n",
    "##\n",
    "df_marketing = pd.read_csv( path + 'marketing.csv' )\n",
    "df_marketing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders_cust, df_marketing, on = 'CID' )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape\n",
    "##\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check number of unique CIDs\n",
    "##\n",
    "x = len( df_orders.CID.unique() )\n",
    "print( 'Number of unique CIDs: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution II.7\n",
    "\n",
    "[Return to Exercise II.7](#Exercise-II.7)\n",
    "\n",
    "Using your merged orders/customers DataFrame, *df*, create a summary statistics display.  What is the skewness of the Total Discount (Tdisc)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Discount is slightly left skewed since the distance between Q! and the median is bigger than the distance between Q3 qnd the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.1\n",
    "\n",
    "[Return to Exercise III.1](#Exercise-III.1)\n",
    "\n",
    "Examine the distribution of pocket price using a histogram.  What can you conclude?  Redo using a log transformation.  Now what do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot( df.Pprice )\n",
    "ax.set( title = \"Pocket Price Distribution\", \n",
    "       xlabel = 'Pocket Price',\n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot( np.log1p( df.Pprice) )\n",
    "ax.set( title = \"Pocket Price Distribution\\nLog Scale\", \n",
    "       xlabel = 'Pocket Price (Natural Log)',\n",
    "       ylabel = 'Proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.2\n",
    "\n",
    "[Return to Exercise III.2](#Exercise-III.2)\n",
    "\n",
    "Check the distribution of the pocket price by marketing region, loyalty program membership, and buyer rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot( x = \"Region\", y = \"Pprice\", data = df )\n",
    "ax.set( title = 'Pocket Price Distribution\\nMarketing Region', \n",
    "       xlabel = 'Marketing Region',\n",
    "      ylabel = 'Pocket Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot( x = \"loyaltyProgram\", y = \"Pprice\", data = df )\n",
    "ax.set( title = 'Pocket Price Distribution\\nLoyalty Program Membership', \n",
    "       xlabel = 'Loyalty Program Membership',\n",
    "      ylabel = 'Pocket Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot( x = \"buyerRating\", y = \"Pprice\", data = df )\n",
    "ax.set( title = 'Pocket Price Distribution\\nLBuyer Rating', \n",
    "       xlabel = 'Buyer Rating',\n",
    "      ylabel = 'Pocket Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.3\n",
    "\n",
    "Create a random sample of $n = 1000$ and plot unit sales vs. total discounts (Tdics).  What do you include?\n",
    "\n",
    "[Return to Exercise III.3](#Exercise-III.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Draw a random sample of size n = 1000\n",
    "## Put the sample in a new DataFrame.\n",
    "##\n",
    "smpl = df.sample( n = 1000, random_state = 1234 )\n",
    "##\n",
    "## Plot the data using the random sample\n",
    "##\n",
    "ax = sns.regplot( x = 'Tdisc', y = 'Usales', data = smpl )\n",
    "ax.set( title = 'Unit Sales vs. Total Discounts\\nRandom Sample\\nn = 1000', \n",
    "       ylabel = 'Unit Sales', xlabel = 'Total Discounts' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution III.4\n",
    "\n",
    "[Return to Exercise III.4](#Exercise-III.4)\n",
    "\n",
    "Study the relationship between Total Discount (*Tdisc*) and the pocket price (*Pprice*).  Use a random sample of $n = 200$, a Lowess smooth, and omit the scatter points.  Let the pocket price be on the vertical axis.  What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a sample of n = 200\n",
    "##\n",
    "smpl = df.sample( n = 200, random_state = 1234 )\n",
    "##\n",
    "## Plot\n",
    "##\n",
    "ax = sns.regplot( x = 'Tdisc', y = 'Pprice', data = smpl, lowess = True, scatter = False )\n",
    "ax.set( title = 'Pocket Price vs Total Discount\\nRandom Sample\\nn = 200\\nWith Lowess Smooth', \n",
    "       xlabel = 'Total Discount', ylabel = 'Pocket Price' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.1\n",
    "\n",
    "Merge the X and y testing data sets for predicting.\n",
    "\n",
    "[Return to Exercise IV.1](#Exercise-IV.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data sets for predicting.\n",
    "## Use an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable Usales.\n",
    "##\n",
    "yy = pd.DataFrame( { 'Usales':y_test } )\n",
    "test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( test.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.2\n",
    "\n",
    "Add log Usales and log Pprice to the testing data set.\n",
    "\n",
    "[Return to Exercise IV.2](#Exercise-IV.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for the testing data\n",
    "##\n",
    "test[ 'log_Usales' ] = np.log1p( test.Usales )\n",
    "test[ 'log_Pprice' ] = np.log1p( test.Pprice )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( test.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Testng Data Set Shape:\\n {}'.format( test.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.3\n",
    "\n",
    "[Return to Exercise IV.3](#Exercise-IV.3)\n",
    "\n",
    "Estimate a new OLS model by adding the buyer rating to the above model. Name your model regE01.  Interpret your results.  Is the buyer rating important for sales?\n",
    "\n",
    "Hint: Buyer rating is categorical so you have to create dummies for the rating.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "## The formula uses a ~ to separate the left-hand side from the right-hand side\n",
    "## of a model and a + to add columns to the right-hand side.  A - sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_Usales ~ log_Pprice + Ddisc + Odisc + Cdisc + Pdisc + C( Region ) + C( buyerRating )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your models\n",
    "##      This numbering includes an \"E\" for \"Exercise\"\n",
    "##\n",
    "regE01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( regE01.summary() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The buyer rating is highly insignificant so this variable is not important and can be omitted in a next iteration of estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.4\n",
    "\n",
    "[Return to Exercise IV.4](#Exercise-IV.4)\n",
    "\n",
    "Test the Null Hypothesis that all the buyer rating estimated parameters are zero.  That is, there is no difference among the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## F-test for the buyer ratings\n",
    "##\n",
    "hypothesis = ' ( C(buyerRating)[T.Good] = 0, C(buyerRating)[T.Poor] = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = regE01.f_test( hypothesis )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "pval = round( float( f_test.pvalue ), 2 )\n",
    "##\n",
    "## Print results\n",
    "##\n",
    "print( 'p-value for F-Test: {}'.format( pval ) )\n",
    "if pval < 0.05:\n",
    "    print( 'Significant so reject H0' )\n",
    "else:\n",
    "    print( 'Insignificant so do not reject H0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Buyer Rating is highly insignificant (i.e., do not reject the Null Hypothesis) because the p-value is 0.68 which is greater than 0.05.  We already know this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution IV.5\n",
    "\n",
    "Create a scenario with the following settings for your model with buyer Rating (regE01):\n",
    "\n",
    "- 'Pprice': [ 2.50 ]\n",
    "- 'Ddisc': [ 0.03 ]\n",
    "- 'Odisc': [ 0.03 ]\n",
    "- 'Cdisc': [ 0.03 ]\n",
    "- 'Pdisc': [ 0.03 ]\n",
    "- 'buyerRating': [ 'Poor' ]\n",
    "- 'Region': [ 'South' ]\n",
    "\n",
    "[Return to Exercise IV.5](#Exercise-IV.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dict\n",
    "##\n",
    "data = {\n",
    "         'Pprice': [ 2.50 ],\n",
    "         'Ddisc': [ 0.03 ],\n",
    "         'Odisc': [ 0.03 ],\n",
    "         'Cdisc': [ 0.03 ],\n",
    "         'Pdisc': [ 0.03 ],\n",
    "         'buyerRating': [ 'Poor' ],\n",
    "         'Region': [ 'South' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_Pprice',\n",
    "                value = np.log1p( scenario.Pprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = regE01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
